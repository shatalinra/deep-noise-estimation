2021-05-22 13:33:25,133: <INFO> Trying simple model
2021-05-22 13:33:49,697: <INFO> Training data size 48699
2021-05-22 13:33:49,698: <INFO> Training model: attempt 0
2021-05-22 13:33:50,535: <INFO> Model: "simple"
2021-05-22 13:33:50,535: <INFO> _________________________________________________________________
2021-05-22 13:33:50,536: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-22 13:33:50,536: <INFO> =================================================================
2021-05-22 13:33:50,538: <INFO> conv-1 (Conv2D)              (None, 16, 16, 8)         104       
2021-05-22 13:33:50,539: <INFO> _________________________________________________________________
2021-05-22 13:33:50,540: <INFO> conv-1-leaky (LeakyReLU)     (None, 16, 16, 8)         0         
2021-05-22 13:33:50,540: <INFO> _________________________________________________________________
2021-05-22 13:33:50,541: <INFO> conv-2 (Conv2D)              (None, 8, 8, 16)          528       
2021-05-22 13:33:50,542: <INFO> _________________________________________________________________
2021-05-22 13:33:50,543: <INFO> conv-2-leaky (LeakyReLU)     (None, 8, 8, 16)          0         
2021-05-22 13:33:50,543: <INFO> _________________________________________________________________
2021-05-22 13:33:50,546: <INFO> conv-3 (Conv2D)              (None, 4, 4, 16)          1040      
2021-05-22 13:33:50,547: <INFO> _________________________________________________________________
2021-05-22 13:33:50,548: <INFO> conv-3-leaky (LeakyReLU)     (None, 4, 4, 16)          0         
2021-05-22 13:33:50,548: <INFO> _________________________________________________________________
2021-05-22 13:33:50,549: <INFO> conv-4 (Conv2D)              (None, 2, 2, 1)           65        
2021-05-22 13:33:50,550: <INFO> _________________________________________________________________
2021-05-22 13:33:50,551: <INFO> conv-4-leaky (LeakyReLU)     (None, 2, 2, 1)           0         
2021-05-22 13:33:50,551: <INFO> _________________________________________________________________
2021-05-22 13:33:50,553: <INFO> conv-5 (Conv2D)              (None, 1, 1, 10)          50        
2021-05-22 13:33:50,554: <INFO> _________________________________________________________________
2021-05-22 13:33:50,555: <INFO> conv-5-leaky (LeakyReLU)     (None, 1, 1, 10)          0         
2021-05-22 13:33:50,556: <INFO> _________________________________________________________________
2021-05-22 13:33:50,558: <INFO> conv-4-softmax (Softmax)     (None, 1, 1, 10)          0         
2021-05-22 13:33:50,558: <INFO> =================================================================
2021-05-22 13:33:50,561: <INFO> Total params: 1,787
2021-05-22 13:33:50,562: <INFO> Trainable params: 1,787
2021-05-22 13:33:50,562: <INFO> Non-trainable params: 0
2021-05-22 13:33:50,562: <INFO> _________________________________________________________________
2021-05-22 13:33:56,399: <INFO> Epoch 0, loss 2.202843, change 2.202843, grad norm 0.270511, lr 0.004000
2021-05-22 13:34:05,168: <INFO> Epoch 5, loss 1.138587, change 1.064256, grad norm 8.231253, lr 0.004000
2021-05-22 13:34:13,950: <INFO> Epoch 10, loss 0.930110, change 0.208477, grad norm 50.016281, lr 0.004000
2021-05-22 13:34:22,773: <INFO> Epoch 15, loss 0.690229, change 0.239881, grad norm 1.945213, lr 0.004000
2021-05-22 13:34:31,602: <INFO> Epoch 20, loss 0.650124, change 0.040105, grad norm 27.869240, lr 0.004000
2021-05-22 13:34:40,335: <INFO> Epoch 25, loss 0.513934, change 0.136190, grad norm 3.037658, lr 0.004000
2021-05-22 13:34:49,184: <INFO> Epoch 30, loss 0.447520, change 0.066414, grad norm 29.763559, lr 0.004000
2021-05-22 13:34:58,016: <INFO> Epoch 35, loss 0.365948, change 0.081573, grad norm 9.072144, lr 0.004000
2021-05-22 13:35:06,750: <INFO> Epoch 40, loss 0.302723, change 0.063225, grad norm 14.029410, lr 0.004000
2021-05-22 13:35:15,496: <INFO> Epoch 45, loss 0.275105, change 0.027618, grad norm 3.643123, lr 0.004000
2021-05-22 13:35:24,313: <INFO> Epoch 50, loss 0.240292, change 0.034813, grad norm 9.606348, lr 0.004000
2021-05-22 13:35:33,059: <INFO> Epoch 55, loss 0.231485, change 0.008807, grad norm 10.178584, lr 0.004000
2021-05-22 13:35:41,796: <INFO> Epoch 60, loss 0.210821, change 0.020664, grad norm 4.738532, lr 0.004000
2021-05-22 13:35:50,551: <INFO> Epoch 65, loss 0.267660, change -0.056839, grad norm 31.833048, lr 0.004000
2021-05-22 13:35:59,263: <INFO> Epoch 70, loss 0.223716, change 0.043944, grad norm 18.634314, lr 0.004000
2021-05-22 13:36:08,040: <INFO> Epoch 75, loss 0.197232, change 0.026483, grad norm 4.701364, lr 0.004000
2021-05-22 13:36:16,814: <INFO> Epoch 80, loss 0.233034, change -0.035802, grad norm 14.224051, lr 0.004000
2021-05-22 13:36:25,587: <INFO> Epoch 85, loss 0.166677, change 0.066357, grad norm 7.663928, lr 0.004000
2021-05-22 13:36:34,315: <INFO> Epoch 90, loss 0.152711, change 0.013966, grad norm 5.661604, lr 0.004000
2021-05-22 13:36:43,079: <INFO> Epoch 95, loss 0.144738, change 0.007973, grad norm 5.747980, lr 0.004000
2021-05-22 13:36:51,855: <INFO> Epoch 100, loss 0.139742, change 0.004996, grad norm 7.263876, lr 0.004000
2021-05-22 13:37:00,613: <INFO> Epoch 105, loss 0.200335, change -0.060593, grad norm 15.252185, lr 0.004000
2021-05-22 13:37:09,510: <INFO> Epoch 110, loss 0.117722, change 0.082613, grad norm 2.671443, lr 0.004000
2021-05-22 13:37:18,626: <INFO> Epoch 115, loss 0.120307, change -0.002584, grad norm 3.736541, lr 0.004000
2021-05-22 13:37:27,455: <INFO> Epoch 120, loss 0.116523, change 0.003784, grad norm 6.983450, lr 0.004000
2021-05-22 13:37:36,953: <INFO> Epoch 125, loss 0.099681, change 0.016842, grad norm 1.590718, lr 0.004000
2021-05-22 13:37:45,283: <INFO> Epoch 130, loss 0.091433, change 0.008248, grad norm 1.345069, lr 0.004000
2021-05-22 13:37:53,625: <INFO> Epoch 135, loss 0.103995, change -0.012562, grad norm 6.540932, lr 0.004000
2021-05-22 13:38:06,474: <INFO> Epoch 140, loss 0.129031, change -0.025036, grad norm 11.884051, lr 0.004000
2021-05-22 13:38:15,728: <INFO> Epoch 145, loss 0.088835, change 0.040196, grad norm 5.172153, lr 0.004000
2021-05-22 13:38:24,721: <INFO> Epoch 150, loss 0.083872, change 0.004963, grad norm 4.297642, lr 0.004000
2021-05-22 13:38:33,755: <INFO> Epoch 155, loss 0.079197, change 0.004675, grad norm 4.415173, lr 0.004000
2021-05-22 13:38:42,713: <INFO> Epoch 160, loss 0.067903, change 0.011294, grad norm 0.686839, lr 0.004000
2021-05-22 13:38:51,608: <INFO> Epoch 165, loss 0.091266, change -0.023363, grad norm 6.547553, lr 0.004000
2021-05-22 13:39:00,545: <INFO> Epoch 170, loss 0.070375, change 0.020891, grad norm 0.963569, lr 0.004000
2021-05-22 13:39:09,480: <INFO> Epoch 175, loss 0.072845, change -0.002470, grad norm 3.919506, lr 0.004000
2021-05-22 13:39:18,588: <INFO> Epoch 180, loss 0.072405, change 0.000440, grad norm 1.331684, lr 0.004000
2021-05-22 13:39:28,214: <INFO> Epoch 185, loss 0.061094, change 0.011312, grad norm 0.822910, lr 0.004000
2021-05-22 13:39:37,745: <INFO> Epoch 190, loss 0.076530, change -0.015436, grad norm 3.395444, lr 0.004000
2021-05-22 13:39:47,195: <INFO> Epoch 195, loss 0.062478, change 0.014052, grad norm 0.948559, lr 0.004000
2021-05-22 13:39:56,841: <INFO> Epoch 200, loss 0.069348, change -0.006870, grad norm 3.811342, lr 0.004000
2021-05-22 13:40:06,076: <INFO> Epoch 205, loss 0.073524, change -0.004176, grad norm 5.162300, lr 0.004000
2021-05-22 13:40:14,971: <INFO> Epoch 210, loss 0.058677, change 0.014848, grad norm 1.091063, lr 0.004000
2021-05-22 13:40:23,802: <INFO> Epoch 215, loss 0.077051, change -0.018375, grad norm 3.525453, lr 0.004000
2021-05-22 13:40:32,725: <INFO> Epoch 220, loss 0.062076, change 0.014975, grad norm 4.373125, lr 0.004000
2021-05-22 13:40:41,576: <INFO> Epoch 225, loss 0.049377, change 0.012699, grad norm 0.863660, lr 0.004000
2021-05-22 13:40:50,484: <INFO> Epoch 230, loss 0.056667, change -0.007290, grad norm 1.297864, lr 0.004000
2021-05-22 13:40:59,305: <INFO> Epoch 235, loss 0.050046, change 0.006621, grad norm 0.888390, lr 0.004000
2021-05-22 13:41:08,516: <INFO> Epoch 240, loss 0.053066, change -0.003019, grad norm 2.608564, lr 0.004000
2021-05-22 13:41:17,455: <INFO> Epoch 245, loss 0.057001, change -0.003935, grad norm 3.979746, lr 0.004000
2021-05-22 13:41:26,317: <INFO> Epoch 250, loss 0.060816, change -0.003815, grad norm 2.872052, lr 0.004000
2021-05-22 13:41:35,287: <INFO> Epoch 255, loss 0.051282, change 0.009534, grad norm 2.945476, lr 0.004000
2021-05-22 13:41:44,096: <INFO> Epoch 260, loss 0.057757, change -0.006475, grad norm 3.830990, lr 0.004000
2021-05-22 13:41:52,952: <INFO> Epoch 265, loss 0.047579, change 0.010178, grad norm 0.893150, lr 0.004000
2021-05-22 13:42:01,856: <INFO> Epoch 270, loss 0.049335, change -0.001756, grad norm 1.837063, lr 0.004000
2021-05-22 13:42:10,683: <INFO> Epoch 275, loss 0.043123, change 0.006212, grad norm 1.484106, lr 0.004000
2021-05-22 13:42:19,479: <INFO> Epoch 280, loss 0.046190, change -0.003068, grad norm 1.697821, lr 0.004000
2021-05-22 13:42:28,374: <INFO> Epoch 285, loss 0.059174, change -0.012984, grad norm 4.360970, lr 0.004000
2021-05-22 13:42:37,204: <INFO> Epoch 290, loss 0.044729, change 0.014444, grad norm 1.492463, lr 0.004000
2021-05-22 13:42:46,030: <INFO> Epoch 295, loss 0.073647, change -0.028918, grad norm 4.179732, lr 0.004000
2021-05-22 13:42:52,994: <INFO> Training model: attempt 1
2021-05-22 13:42:53,135: <INFO> Model: "simple"
2021-05-22 13:42:53,135: <INFO> _________________________________________________________________
2021-05-22 13:42:53,135: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-22 13:42:53,136: <INFO> =================================================================
2021-05-22 13:42:53,137: <INFO> conv-1 (Conv2D)              (None, 16, 16, 8)         104       
2021-05-22 13:42:53,138: <INFO> _________________________________________________________________
2021-05-22 13:42:53,138: <INFO> conv-1-leaky (LeakyReLU)     (None, 16, 16, 8)         0         
2021-05-22 13:42:53,139: <INFO> _________________________________________________________________
2021-05-22 13:42:53,140: <INFO> conv-2 (Conv2D)              (None, 8, 8, 16)          528       
2021-05-22 13:42:53,140: <INFO> _________________________________________________________________
2021-05-22 13:42:53,141: <INFO> conv-2-leaky (LeakyReLU)     (None, 8, 8, 16)          0         
2021-05-22 13:42:53,141: <INFO> _________________________________________________________________
2021-05-22 13:42:53,142: <INFO> conv-3 (Conv2D)              (None, 4, 4, 16)          1040      
2021-05-22 13:42:53,143: <INFO> _________________________________________________________________
2021-05-22 13:42:53,144: <INFO> conv-3-leaky (LeakyReLU)     (None, 4, 4, 16)          0         
2021-05-22 13:42:53,144: <INFO> _________________________________________________________________
2021-05-22 13:42:53,145: <INFO> conv-4 (Conv2D)              (None, 2, 2, 1)           65        
2021-05-22 13:42:53,146: <INFO> _________________________________________________________________
2021-05-22 13:42:53,147: <INFO> conv-4-leaky (LeakyReLU)     (None, 2, 2, 1)           0         
2021-05-22 13:42:53,147: <INFO> _________________________________________________________________
2021-05-22 13:42:53,149: <INFO> conv-5 (Conv2D)              (None, 1, 1, 10)          50        
2021-05-22 13:42:53,149: <INFO> _________________________________________________________________
2021-05-22 13:42:53,150: <INFO> conv-5-leaky (LeakyReLU)     (None, 1, 1, 10)          0         
2021-05-22 13:42:53,151: <INFO> _________________________________________________________________
2021-05-22 13:42:53,152: <INFO> conv-4-softmax (Softmax)     (None, 1, 1, 10)          0         
2021-05-22 13:42:53,152: <INFO> =================================================================
2021-05-22 13:42:53,155: <INFO> Total params: 1,787
2021-05-22 13:42:53,155: <INFO> Trainable params: 1,787
2021-05-22 13:42:53,156: <INFO> Non-trainable params: 0
2021-05-22 13:42:53,156: <INFO> _________________________________________________________________
2021-05-22 13:42:55,926: <INFO> Epoch 0, loss 2.200840, change 2.200840, grad norm 0.164818, lr 0.004000
2021-05-22 13:43:04,799: <INFO> Epoch 5, loss 1.727017, change 0.473823, grad norm 5.236400, lr 0.004000
2021-05-22 13:43:13,619: <INFO> Epoch 10, loss 0.869455, change 0.857562, grad norm 15.258925, lr 0.004000
2021-05-22 13:43:22,736: <INFO> Epoch 15, loss 0.682330, change 0.187124, grad norm 5.562417, lr 0.004000
2021-05-22 13:43:33,445: <INFO> Epoch 20, loss 0.648179, change 0.034151, grad norm 54.992489, lr 0.004000
2021-05-22 13:43:43,056: <INFO> Epoch 25, loss 0.481226, change 0.166953, grad norm 5.143373, lr 0.004000
2021-05-22 13:43:52,043: <INFO> Epoch 30, loss 0.431511, change 0.049716, grad norm 27.945183, lr 0.004000
2021-05-22 13:44:00,875: <INFO> Epoch 35, loss 0.358839, change 0.072671, grad norm 6.834354, lr 0.004000
2021-05-22 13:44:09,752: <INFO> Epoch 40, loss 0.351851, change 0.006988, grad norm 23.864334, lr 0.004000
2021-05-22 13:44:20,004: <INFO> Epoch 45, loss 0.309291, change 0.042560, grad norm 3.950002, lr 0.004000
2021-05-22 13:44:29,456: <INFO> Epoch 50, loss 0.234756, change 0.074535, grad norm 9.427496, lr 0.004000
2021-05-22 13:44:37,495: <INFO> Epoch 55, loss 0.214033, change 0.020723, grad norm 23.301439, lr 0.004000
2021-05-22 13:44:45,621: <INFO> Epoch 60, loss 0.202885, change 0.011148, grad norm 31.063591, lr 0.004000
2021-05-22 13:44:53,574: <INFO> Epoch 65, loss 0.173192, change 0.029692, grad norm 2.803511, lr 0.004000
2021-05-22 13:45:01,838: <INFO> Epoch 70, loss 0.161469, change 0.011724, grad norm 15.155643, lr 0.004000
2021-05-22 13:45:09,827: <INFO> Epoch 75, loss 0.154438, change 0.007031, grad norm 10.789824, lr 0.004000
2021-05-22 13:45:17,811: <INFO> Epoch 80, loss 0.292124, change -0.137686, grad norm 69.378845, lr 0.004000
2021-05-22 13:45:25,859: <INFO> Epoch 85, loss 0.138153, change 0.153971, grad norm 12.822630, lr 0.004000
2021-05-22 13:45:33,859: <INFO> Epoch 90, loss 0.137434, change 0.000719, grad norm 8.166613, lr 0.004000
2021-05-22 13:45:42,121: <INFO> Epoch 95, loss 0.127732, change 0.009702, grad norm 9.824956, lr 0.004000
2021-05-22 13:45:50,663: <INFO> Epoch 100, loss 0.137962, change -0.010230, grad norm 8.982557, lr 0.004000
2021-05-22 13:45:58,657: <INFO> Epoch 105, loss 0.122836, change 0.015126, grad norm 14.416680, lr 0.004000
2021-05-22 13:46:06,616: <INFO> Epoch 110, loss 0.126925, change -0.004089, grad norm 8.052517, lr 0.004000
2021-05-22 13:46:14,663: <INFO> Epoch 115, loss 0.116818, change 0.010108, grad norm 8.368771, lr 0.004000
2021-05-22 13:46:22,649: <INFO> Epoch 120, loss 0.145140, change -0.028322, grad norm 24.543079, lr 0.004000
2021-05-22 13:46:30,667: <INFO> Epoch 125, loss 0.131354, change 0.013785, grad norm 20.308414, lr 0.004000
2021-05-22 13:46:39,142: <INFO> Epoch 130, loss 0.142248, change -0.010893, grad norm 16.811163, lr 0.004000
2021-05-22 13:46:47,895: <INFO> Epoch 135, loss 0.104742, change 0.037506, grad norm 4.831066, lr 0.004000
2021-05-22 13:46:56,383: <INFO> Epoch 140, loss 0.099216, change 0.005526, grad norm 2.594105, lr 0.004000
2021-05-22 13:47:04,687: <INFO> Epoch 145, loss 0.115316, change -0.016101, grad norm 19.299900, lr 0.004000
2021-05-22 13:47:12,797: <INFO> Epoch 150, loss 0.152248, change -0.036932, grad norm 47.557579, lr 0.004000
2021-05-22 13:47:20,740: <INFO> Epoch 155, loss 0.090263, change 0.061985, grad norm 5.679417, lr 0.004000
2021-05-22 13:47:28,638: <INFO> Epoch 160, loss 0.092073, change -0.001810, grad norm 8.076368, lr 0.004000
2021-05-22 13:47:36,832: <INFO> Epoch 165, loss 0.089985, change 0.002088, grad norm 9.412582, lr 0.004000
2021-05-22 13:47:44,834: <INFO> Epoch 170, loss 0.138503, change -0.048518, grad norm 30.010937, lr 0.004000
2021-05-22 13:47:52,814: <INFO> Epoch 175, loss 0.098613, change 0.039891, grad norm 18.128788, lr 0.004000
2021-05-22 13:48:00,723: <INFO> Epoch 180, loss 0.078553, change 0.020060, grad norm 2.862920, lr 0.004000
2021-05-22 13:48:08,867: <INFO> Epoch 185, loss 0.097924, change -0.019372, grad norm 14.635041, lr 0.004000
2021-05-22 13:48:17,318: <INFO> Epoch 190, loss 0.071057, change 0.026867, grad norm 4.171662, lr 0.004000
2021-05-22 13:48:25,650: <INFO> Epoch 195, loss 0.072785, change -0.001727, grad norm 2.071533, lr 0.004000
2021-05-22 13:48:33,854: <INFO> Epoch 200, loss 0.110692, change -0.037908, grad norm 19.096508, lr 0.004000
2021-05-22 13:48:42,424: <INFO> Epoch 205, loss 0.091499, change 0.019193, grad norm 13.524013, lr 0.004000
2021-05-22 13:48:51,556: <INFO> Epoch 210, loss 0.080687, change 0.010812, grad norm 8.941783, lr 0.004000
2021-05-22 13:49:00,516: <INFO> Epoch 215, loss 0.064109, change 0.016578, grad norm 4.362548, lr 0.004000
2021-05-22 13:49:09,396: <INFO> Epoch 220, loss 0.065442, change -0.001333, grad norm 7.926851, lr 0.004000
2021-05-22 13:49:18,205: <INFO> Epoch 225, loss 0.080335, change -0.014893, grad norm 16.497162, lr 0.004000
2021-05-22 13:49:27,015: <INFO> Epoch 230, loss 0.073574, change 0.006761, grad norm 6.392833, lr 0.004000
2021-05-22 13:49:35,849: <INFO> Epoch 235, loss 0.056790, change 0.016784, grad norm 3.821739, lr 0.004000
2021-05-22 13:49:44,642: <INFO> Epoch 240, loss 0.086432, change -0.029642, grad norm 21.419138, lr 0.004000
2021-05-22 13:49:53,479: <INFO> Epoch 245, loss 0.060013, change 0.026419, grad norm 5.196310, lr 0.004000
2021-05-22 13:50:02,279: <INFO> Epoch 250, loss 0.065542, change -0.005529, grad norm 8.730551, lr 0.004000
2021-05-22 13:50:11,137: <INFO> Epoch 255, loss 0.062976, change 0.002566, grad norm 9.522714, lr 0.004000
2021-05-22 13:50:19,890: <INFO> Epoch 260, loss 0.052808, change 0.010168, grad norm 1.797142, lr 0.004000
2021-05-22 13:50:28,711: <INFO> Epoch 265, loss 0.053653, change -0.000845, grad norm 5.846743, lr 0.004000
2021-05-22 13:50:37,539: <INFO> Epoch 270, loss 0.049653, change 0.004000, grad norm 2.798675, lr 0.004000
2021-05-22 13:50:46,311: <INFO> Epoch 275, loss 0.046988, change 0.002665, grad norm 0.556499, lr 0.004000
2021-05-22 13:50:55,168: <INFO> Epoch 280, loss 0.052586, change -0.005598, grad norm 3.107048, lr 0.004000
2021-05-22 13:51:04,047: <INFO> Epoch 285, loss 0.056871, change -0.004285, grad norm 10.392447, lr 0.004000
2021-05-22 13:51:14,082: <INFO> Epoch 290, loss 0.066692, change -0.009821, grad norm 3.829192, lr 0.004000
2021-05-22 13:51:23,731: <INFO> Epoch 295, loss 0.062188, change 0.004504, grad norm 13.625191, lr 0.004000
2021-05-22 13:51:30,883: <INFO> Training model: attempt 2
2021-05-22 13:51:31,020: <INFO> Model: "simple"
2021-05-22 13:51:31,022: <INFO> _________________________________________________________________
2021-05-22 13:51:31,023: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-22 13:51:31,026: <INFO> =================================================================
2021-05-22 13:51:31,031: <INFO> conv-1 (Conv2D)              (None, 16, 16, 8)         104       
2021-05-22 13:51:31,032: <INFO> _________________________________________________________________
2021-05-22 13:51:31,033: <INFO> conv-1-leaky (LeakyReLU)     (None, 16, 16, 8)         0         
2021-05-22 13:51:31,034: <INFO> _________________________________________________________________
2021-05-22 13:51:31,035: <INFO> conv-2 (Conv2D)              (None, 8, 8, 16)          528       
2021-05-22 13:51:31,035: <INFO> _________________________________________________________________
2021-05-22 13:51:31,036: <INFO> conv-2-leaky (LeakyReLU)     (None, 8, 8, 16)          0         
2021-05-22 13:51:31,036: <INFO> _________________________________________________________________
2021-05-22 13:51:31,037: <INFO> conv-3 (Conv2D)              (None, 4, 4, 16)          1040      
2021-05-22 13:51:31,037: <INFO> _________________________________________________________________
2021-05-22 13:51:31,038: <INFO> conv-3-leaky (LeakyReLU)     (None, 4, 4, 16)          0         
2021-05-22 13:51:31,039: <INFO> _________________________________________________________________
2021-05-22 13:51:31,040: <INFO> conv-4 (Conv2D)              (None, 2, 2, 1)           65        
2021-05-22 13:51:31,041: <INFO> _________________________________________________________________
2021-05-22 13:51:31,042: <INFO> conv-4-leaky (LeakyReLU)     (None, 2, 2, 1)           0         
2021-05-22 13:51:31,043: <INFO> _________________________________________________________________
2021-05-22 13:51:31,045: <INFO> conv-5 (Conv2D)              (None, 1, 1, 10)          50        
2021-05-22 13:51:31,045: <INFO> _________________________________________________________________
2021-05-22 13:51:31,046: <INFO> conv-5-leaky (LeakyReLU)     (None, 1, 1, 10)          0         
2021-05-22 13:51:31,046: <INFO> _________________________________________________________________
2021-05-22 13:51:31,048: <INFO> conv-4-softmax (Softmax)     (None, 1, 1, 10)          0         
2021-05-22 13:51:31,048: <INFO> =================================================================
2021-05-22 13:51:31,051: <INFO> Total params: 1,787
2021-05-22 13:51:31,051: <INFO> Trainable params: 1,787
2021-05-22 13:51:31,052: <INFO> Non-trainable params: 0
2021-05-22 13:51:31,052: <INFO> _________________________________________________________________
2021-05-22 13:51:33,926: <INFO> Epoch 0, loss 1.924171, change 1.924171, grad norm 1.191320, lr 0.004000
2021-05-22 13:51:42,786: <INFO> Epoch 5, loss 0.643079, change 1.281092, grad norm 19.221790, lr 0.004000
2021-05-22 13:51:51,736: <INFO> Epoch 10, loss 0.623081, change 0.019998, grad norm 99.929695, lr 0.004000
2021-05-22 13:52:00,652: <INFO> Epoch 15, loss 0.355879, change 0.267201, grad norm 46.352730, lr 0.004000
2021-05-22 13:52:09,513: <INFO> Epoch 20, loss 0.290535, change 0.065345, grad norm 26.136557, lr 0.004000
2021-05-22 13:52:18,407: <INFO> Epoch 25, loss 0.233684, change 0.056851, grad norm 12.897411, lr 0.004000
2021-05-22 13:52:27,304: <INFO> Epoch 30, loss 0.186553, change 0.047131, grad norm 12.954211, lr 0.004000
2021-05-22 13:52:36,225: <INFO> Epoch 35, loss 0.333902, change -0.147349, grad norm 85.234512, lr 0.004000
2021-05-22 13:52:45,118: <INFO> Epoch 40, loss 0.186804, change 0.147098, grad norm 31.685862, lr 0.004000
2021-05-22 13:52:53,910: <INFO> Epoch 45, loss 0.142628, change 0.044176, grad norm 6.539340, lr 0.004000
2021-05-22 13:53:02,703: <INFO> Epoch 50, loss 0.168349, change -0.025722, grad norm 30.240650, lr 0.004000
2021-05-22 13:53:11,515: <INFO> Epoch 55, loss 0.618984, change -0.450634, grad norm 164.410843, lr 0.004000
2021-05-22 13:53:20,307: <INFO> Epoch 60, loss 0.122055, change 0.496929, grad norm 10.643367, lr 0.004000
2021-05-22 13:53:29,178: <INFO> Epoch 65, loss 0.103198, change 0.018856, grad norm 2.926166, lr 0.004000
2021-05-22 13:53:37,983: <INFO> Epoch 70, loss 0.141369, change -0.038171, grad norm 38.499599, lr 0.004000
2021-05-22 13:53:46,847: <INFO> Epoch 75, loss 0.131730, change 0.009639, grad norm 21.983932, lr 0.004000
2021-05-22 13:53:55,749: <INFO> Epoch 80, loss 0.112908, change 0.018822, grad norm 7.635537, lr 0.004000
2021-05-22 13:54:04,671: <INFO> Epoch 85, loss 0.092478, change 0.020430, grad norm 6.787786, lr 0.004000
2021-05-22 13:54:13,561: <INFO> Epoch 90, loss 0.091593, change 0.000885, grad norm 7.151596, lr 0.004000
2021-05-22 13:54:22,437: <INFO> Epoch 95, loss 0.088402, change 0.003192, grad norm 5.834591, lr 0.004000
2021-05-22 13:54:31,306: <INFO> Epoch 100, loss 0.090030, change -0.001629, grad norm 12.228371, lr 0.004000
2021-05-22 13:54:40,180: <INFO> Epoch 105, loss 0.077753, change 0.012278, grad norm 2.764399, lr 0.004000
2021-05-22 13:54:48,986: <INFO> Epoch 110, loss 0.071939, change 0.005814, grad norm 4.254185, lr 0.004000
2021-05-22 13:54:57,766: <INFO> Epoch 115, loss 0.672935, change -0.600996, grad norm 143.086533, lr 0.004000
2021-05-22 13:55:06,817: <INFO> Epoch 120, loss 0.108899, change 0.564035, grad norm 20.034531, lr 0.004000
2021-05-22 13:55:15,680: <INFO> Epoch 125, loss 0.079415, change 0.029484, grad norm 10.464250, lr 0.004000
2021-05-22 13:55:24,540: <INFO> Epoch 130, loss 0.063572, change 0.015843, grad norm 2.106588, lr 0.004000
2021-05-22 13:55:33,406: <INFO> Epoch 135, loss 0.055491, change 0.008081, grad norm 6.378185, lr 0.004000
2021-05-22 13:55:42,213: <INFO> Epoch 140, loss 0.064337, change -0.008846, grad norm 9.954414, lr 0.004000
2021-05-22 13:55:50,978: <INFO> Epoch 145, loss 0.057196, change 0.007141, grad norm 4.922180, lr 0.004000
2021-05-22 13:55:59,789: <INFO> Epoch 150, loss 0.058407, change -0.001211, grad norm 5.597013, lr 0.004000
2021-05-22 13:56:08,646: <INFO> Epoch 155, loss 0.047365, change 0.011042, grad norm 1.790498, lr 0.004000
2021-05-22 13:56:17,512: <INFO> Epoch 160, loss 0.052126, change -0.004761, grad norm 7.566225, lr 0.004000
2021-05-22 13:56:26,397: <INFO> Epoch 165, loss 0.046858, change 0.005268, grad norm 2.927101, lr 0.004000
2021-05-22 13:56:35,295: <INFO> Epoch 170, loss 0.067330, change -0.020472, grad norm 12.057004, lr 0.004000
2021-05-22 13:56:44,174: <INFO> Epoch 175, loss 0.041053, change 0.026277, grad norm 3.736861, lr 0.004000
2021-05-22 13:56:53,061: <INFO> Epoch 180, loss 0.056054, change -0.015000, grad norm 6.179445, lr 0.004000
2021-05-22 13:57:01,977: <INFO> Epoch 185, loss 0.038876, change 0.017178, grad norm 3.805459, lr 0.004000
2021-05-22 13:57:12,049: <INFO> Epoch 190, loss 0.037682, change 0.001193, grad norm 4.237419, lr 0.004000
2021-05-22 13:57:21,623: <INFO> Epoch 195, loss 0.037812, change -0.000129, grad norm 2.518335, lr 0.004000
2021-05-22 13:57:30,550: <INFO> Epoch 200, loss 0.036124, change 0.001687, grad norm 1.938354, lr 0.004000
2021-05-22 13:57:39,555: <INFO> Epoch 205, loss 0.066694, change -0.030570, grad norm 12.181273, lr 0.004000
2021-05-22 13:57:48,496: <INFO> Epoch 210, loss 0.036304, change 0.030390, grad norm 5.382805, lr 0.004000
2021-05-22 13:57:57,474: <INFO> Epoch 215, loss 0.028529, change 0.007775, grad norm 0.731642, lr 0.004000
2021-05-22 13:58:06,466: <INFO> Epoch 220, loss 0.032045, change -0.003516, grad norm 3.296254, lr 0.004000
2021-05-22 13:58:15,363: <INFO> Epoch 225, loss 0.035355, change -0.003310, grad norm 2.844338, lr 0.004000
2021-05-22 13:58:23,737: <INFO> Epoch 230, loss 0.031609, change 0.003746, grad norm 3.214959, lr 0.004000
2021-05-22 13:58:31,772: <INFO> Epoch 235, loss 0.028082, change 0.003527, grad norm 0.674314, lr 0.004000
2021-05-22 13:58:39,757: <INFO> Epoch 240, loss 0.029065, change -0.000982, grad norm 1.867802, lr 0.004000
2021-05-22 13:58:47,807: <INFO> Epoch 245, loss 0.065125, change -0.036060, grad norm 4.102290, lr 0.004000
2021-05-22 13:58:55,810: <INFO> Epoch 250, loss 0.039498, change 0.025627, grad norm 2.133762, lr 0.004000
2021-05-22 13:59:03,797: <INFO> Epoch 255, loss 0.035436, change 0.004062, grad norm 3.083977, lr 0.004000
2021-05-22 13:59:11,850: <INFO> Epoch 260, loss 0.031844, change 0.003592, grad norm 3.066343, lr 0.004000
2021-05-22 13:59:19,905: <INFO> Epoch 265, loss 0.022689, change 0.009155, grad norm 0.531003, lr 0.004000
2021-05-22 13:59:27,920: <INFO> Epoch 270, loss 0.021795, change 0.000894, grad norm 0.704956, lr 0.004000
2021-05-22 13:59:35,952: <INFO> Epoch 275, loss 0.020413, change 0.001382, grad norm 0.182262, lr 0.004000
2021-05-22 13:59:43,912: <INFO> Epoch 280, loss 0.028650, change -0.008237, grad norm 3.008700, lr 0.004000
2021-05-22 13:59:51,887: <INFO> Epoch 285, loss 0.032093, change -0.003443, grad norm 1.301933, lr 0.004000
2021-05-22 13:59:59,843: <INFO> Epoch 290, loss 0.039015, change -0.006922, grad norm 5.115234, lr 0.004000
2021-05-22 14:00:07,810: <INFO> Epoch 295, loss 0.027097, change 0.011917, grad norm 2.647443, lr 0.004000
2021-05-22 14:00:15,323: <WARNING> From C:\Users\Teucros\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\training\tracking\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2021-05-22 14:00:15,351: <WARNING> From C:\Users\Teucros\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\training\tracking\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2021-05-22 14:00:17,656: <INFO> Assets written to: trained_models/simple\assets
2021-05-22 14:00:17,778: <INFO> Best loss is 0.029638
