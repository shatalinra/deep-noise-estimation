2021-05-26 21:29:19,491: <INFO> Trying efficent model
2021-05-26 21:29:21,822: <INFO> Training model: attempt 0
2021-05-26 21:29:21,831: <WARNING> Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.
2021-05-26 21:29:21,866: <INFO> Model: "efficient"
2021-05-26 21:29:21,867: <INFO> _________________________________________________________________
2021-05-26 21:29:21,867: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-26 21:29:21,869: <INFO> =================================================================
2021-05-26 21:29:21,871: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-26 21:29:21,872: <INFO> _________________________________________________________________
2021-05-26 21:29:21,873: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-26 21:29:21,873: <INFO> _________________________________________________________________
2021-05-26 21:29:21,874: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-26 21:29:21,875: <INFO> =================================================================
2021-05-26 21:29:21,876: <INFO> Total params: 12,810
2021-05-26 21:29:21,877: <INFO> Trainable params: 12,810
2021-05-26 21:29:21,877: <INFO> Non-trainable params: 0
2021-05-26 21:29:21,878: <INFO> _________________________________________________________________
2021-05-26 21:30:33,346: <INFO> Epoch 0, loss 1.7873
2021-05-26 21:30:33,347: <INFO> Epoch 1, loss 1.5117
2021-05-26 21:30:33,347: <INFO> Epoch 2, loss 1.3919
2021-05-26 21:30:33,348: <INFO> Epoch 3, loss 1.2712
2021-05-26 21:30:33,348: <INFO> Epoch 4, loss 1.4370
2021-05-26 21:30:33,349: <INFO> Epoch 5, loss 1.6986
2021-05-26 21:30:33,350: <INFO> Epoch 6, loss 1.6867
2021-05-26 21:30:33,350: <INFO> Epoch 7, loss 1.7664
2021-05-26 21:30:33,351: <INFO> Epoch 8, loss 1.7564
2021-05-26 21:30:33,351: <INFO> Epoch 9, loss 1.6452
2021-05-26 21:30:33,351: <INFO> Epoch 10, loss 1.4644
2021-05-26 21:30:33,352: <INFO> Epoch 11, loss 1.1325
2021-05-26 21:30:33,352: <INFO> Epoch 12, loss 1.1978
2021-05-26 21:30:33,352: <INFO> Epoch 13, loss 1.1904
2021-05-26 21:30:33,353: <INFO> Epoch 14, loss 1.0335
2021-05-26 21:30:33,353: <INFO> Epoch 15, loss 0.9804
2021-05-26 21:30:33,354: <INFO> Epoch 16, loss 1.0086
2021-05-26 21:30:33,356: <INFO> Epoch 17, loss 1.1310
2021-05-26 21:30:33,356: <INFO> Epoch 18, loss 0.8990
2021-05-26 21:30:33,357: <INFO> Epoch 19, loss 0.8062
2021-05-26 21:30:33,357: <INFO> Epoch 20, loss 0.7794
2021-05-26 21:30:33,358: <INFO> Epoch 21, loss 0.9026
2021-05-26 21:30:33,358: <INFO> Epoch 22, loss 0.6834
2021-05-26 21:30:33,359: <INFO> Epoch 23, loss 0.7101
2021-05-26 21:30:33,359: <INFO> Epoch 24, loss 0.7018
2021-05-26 21:30:33,359: <INFO> Epoch 25, loss 0.7511
2021-05-26 21:30:33,360: <INFO> Epoch 26, loss 0.7983
2021-05-26 21:30:33,360: <INFO> Epoch 27, loss 0.8747
2021-05-26 21:30:33,361: <INFO> Epoch 28, loss 0.8806
2021-05-26 21:30:33,362: <INFO> Epoch 29, loss 0.8622
2021-05-26 21:30:33,362: <INFO> Epoch 30, loss 0.8553
2021-05-26 21:30:33,363: <INFO> Epoch 31, loss 0.8141
2021-05-26 21:30:33,363: <INFO> Epoch 32, loss 0.7906
2021-05-26 21:30:33,364: <INFO> Epoch 33, loss 0.6815
2021-05-26 21:30:33,364: <INFO> Epoch 34, loss 0.6690
2021-05-26 21:30:33,365: <INFO> Epoch 35, loss 0.5977
2021-05-26 21:30:33,366: <INFO> Epoch 36, loss 0.5200
2021-05-26 21:30:33,367: <INFO> Epoch 37, loss 0.4725
2021-05-26 21:30:33,367: <INFO> Epoch 38, loss 0.5696
2021-05-26 21:30:33,368: <INFO> Epoch 39, loss 0.5689
2021-05-26 21:30:33,369: <INFO> Epoch 40, loss 0.5347
2021-05-26 21:30:33,369: <INFO> Epoch 41, loss 0.4803
2021-05-26 21:30:33,370: <INFO> Epoch 42, loss 0.4937
2021-05-26 21:30:33,371: <INFO> Epoch 43, loss 0.5001
2021-05-26 21:30:33,371: <INFO> Epoch 44, loss 0.4477
2021-05-26 21:30:33,372: <INFO> Epoch 45, loss 0.4189
2021-05-26 21:30:33,372: <INFO> Epoch 46, loss 0.3476
2021-05-26 21:30:33,372: <INFO> Epoch 47, loss 0.3994
2021-05-26 21:30:33,373: <INFO> Epoch 48, loss 0.4196
2021-05-26 21:30:33,373: <INFO> Epoch 49, loss 0.3895
2021-05-26 21:30:33,373: <INFO> Epoch 50, loss 0.3491
2021-05-26 21:30:33,374: <INFO> Epoch 51, loss 0.3383
2021-05-26 21:30:33,374: <INFO> Epoch 52, loss 0.3514
2021-05-26 21:30:33,375: <INFO> Epoch 53, loss 0.3457
2021-05-26 21:30:33,375: <INFO> Epoch 54, loss 0.3226
2021-05-26 21:30:33,375: <INFO> Epoch 55, loss 0.3161
2021-05-26 21:30:33,376: <INFO> Epoch 56, loss 0.2673
2021-05-26 21:30:33,376: <INFO> Epoch 57, loss 0.2697
2021-05-26 21:30:33,376: <INFO> Epoch 58, loss 0.3007
2021-05-26 21:30:33,377: <INFO> Epoch 59, loss 0.2925
2021-05-26 21:30:33,377: <INFO> Epoch 60, loss 0.2527
2021-05-26 21:30:33,377: <INFO> Epoch 61, loss 0.2112
2021-05-26 21:30:33,377: <INFO> Epoch 62, loss 0.2245
2021-05-26 21:30:33,378: <INFO> Epoch 63, loss 0.2328
2021-05-26 21:30:33,378: <INFO> Epoch 64, loss 0.2367
2021-05-26 21:30:33,379: <INFO> Epoch 65, loss 0.2134
2021-05-26 21:30:33,379: <INFO> Epoch 66, loss 0.2206
2021-05-26 21:30:33,379: <INFO> Epoch 67, loss 0.1916
2021-05-26 21:30:33,380: <INFO> Epoch 68, loss 0.1741
2021-05-26 21:30:33,380: <INFO> Epoch 69, loss 0.1776
2021-05-26 21:30:33,380: <INFO> Epoch 70, loss 0.1735
2021-05-26 21:30:33,381: <INFO> Epoch 71, loss 0.1625
2021-05-26 21:30:33,381: <INFO> Epoch 72, loss 0.1462
2021-05-26 21:30:33,381: <INFO> Epoch 73, loss 0.1447
2021-05-26 21:30:33,382: <INFO> Epoch 74, loss 0.1356
2021-05-26 21:30:33,382: <INFO> Epoch 75, loss 0.1283
2021-05-26 21:30:33,383: <INFO> Epoch 76, loss 0.1335
2021-05-26 21:30:33,383: <INFO> Epoch 77, loss 0.1482
2021-05-26 21:30:33,383: <INFO> Epoch 78, loss 0.1718
2021-05-26 21:30:33,384: <INFO> Epoch 79, loss 0.2111
2021-05-26 21:30:33,384: <INFO> Epoch 80, loss 0.2050
2021-05-26 21:30:33,384: <INFO> Epoch 81, loss 0.2308
2021-05-26 21:30:33,384: <INFO> Epoch 82, loss 0.2033
2021-05-26 21:30:33,385: <INFO> Epoch 83, loss 0.2302
2021-05-26 21:30:33,385: <INFO> Epoch 84, loss 0.2618
2021-05-26 21:30:33,385: <INFO> Epoch 85, loss 0.3005
2021-05-26 21:30:33,385: <INFO> Epoch 86, loss 0.2845
2021-05-26 21:30:33,386: <INFO> Epoch 87, loss 0.2520
2021-05-26 21:30:33,386: <INFO> Epoch 88, loss 0.2517
2021-05-26 21:30:33,387: <INFO> Epoch 89, loss 0.2884
2021-05-26 21:30:33,387: <INFO> Epoch 90, loss 0.3025
2021-05-26 21:30:33,388: <INFO> Epoch 91, loss 0.2959
2021-05-26 21:30:33,388: <INFO> Epoch 92, loss 0.3565
2021-05-26 21:30:33,388: <INFO> Epoch 93, loss 0.4791
2021-05-26 21:30:33,389: <INFO> Epoch 94, loss 0.6242
2021-05-26 21:30:33,389: <INFO> Epoch 95, loss 0.6134
2021-05-26 21:30:33,389: <INFO> Epoch 96, loss 0.7697
2021-05-26 21:30:33,390: <INFO> Epoch 97, loss 1.1913
2021-05-26 21:30:33,390: <INFO> Epoch 98, loss 2.0977
2021-05-26 21:30:33,390: <INFO> Epoch 99, loss 1.4431
2021-05-26 21:30:33,391: <INFO> Training model: attempt 1
2021-05-26 21:30:33,400: <WARNING> Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.
2021-05-26 21:30:33,434: <INFO> Model: "efficient"
2021-05-26 21:30:33,434: <INFO> _________________________________________________________________
2021-05-26 21:30:33,435: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-26 21:30:33,437: <INFO> =================================================================
2021-05-26 21:30:33,439: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-26 21:30:33,439: <INFO> _________________________________________________________________
2021-05-26 21:30:33,440: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-26 21:30:33,441: <INFO> _________________________________________________________________
2021-05-26 21:30:33,442: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-26 21:30:33,442: <INFO> =================================================================
2021-05-26 21:30:33,443: <INFO> Total params: 12,810
2021-05-26 21:30:33,444: <INFO> Trainable params: 12,810
2021-05-26 21:30:33,444: <INFO> Non-trainable params: 0
2021-05-26 21:30:33,445: <INFO> _________________________________________________________________
2021-05-26 21:31:02,348: <INFO> Epoch 0, loss 1.7288
2021-05-26 21:31:02,349: <INFO> Epoch 1, loss 1.4212
2021-05-26 21:31:02,349: <INFO> Epoch 2, loss 1.5348
2021-05-26 21:31:02,350: <INFO> Epoch 3, loss 1.3279
2021-05-26 21:31:02,350: <INFO> Epoch 4, loss 1.3968
2021-05-26 21:31:02,350: <INFO> Epoch 5, loss 1.3935
2021-05-26 21:31:02,351: <INFO> Epoch 6, loss 1.5559
2021-05-26 21:31:02,351: <INFO> Epoch 7, loss 1.6160
2021-05-26 21:31:02,352: <INFO> Epoch 8, loss 1.4451
2021-05-26 21:31:02,352: <INFO> Epoch 9, loss 1.4184
2021-05-26 21:31:02,352: <INFO> Epoch 10, loss 1.2441
2021-05-26 21:31:02,353: <INFO> Epoch 11, loss 1.3145
2021-05-26 21:31:02,354: <INFO> Epoch 12, loss 1.1438
2021-05-26 21:31:02,355: <INFO> Epoch 13, loss 1.1403
2021-05-26 21:31:02,355: <INFO> Epoch 14, loss 0.9505
2021-05-26 21:31:02,356: <INFO> Epoch 15, loss 1.0500
2021-05-26 21:31:02,356: <INFO> Epoch 16, loss 1.1432
2021-05-26 21:31:02,356: <INFO> Epoch 17, loss 1.2209
2021-05-26 21:31:02,356: <INFO> Epoch 18, loss 0.9984
2021-05-26 21:31:02,357: <INFO> Epoch 19, loss 0.9476
2021-05-26 21:31:02,357: <INFO> Epoch 20, loss 0.8730
2021-05-26 21:31:02,358: <INFO> Epoch 21, loss 0.8752
2021-05-26 21:31:02,358: <INFO> Epoch 22, loss 0.8732
2021-05-26 21:31:02,359: <INFO> Epoch 23, loss 0.8329
2021-05-26 21:31:02,360: <INFO> Epoch 24, loss 0.8002
2021-05-26 21:31:02,360: <INFO> Epoch 25, loss 0.8460
2021-05-26 21:31:02,361: <INFO> Epoch 26, loss 0.8054
2021-05-26 21:31:02,361: <INFO> Epoch 27, loss 0.7174
2021-05-26 21:31:02,361: <INFO> Epoch 28, loss 0.7228
2021-05-26 21:31:02,362: <INFO> Epoch 29, loss 0.7181
2021-05-26 21:31:02,362: <INFO> Epoch 30, loss 0.7241
2021-05-26 21:31:02,362: <INFO> Epoch 31, loss 0.6591
2021-05-26 21:31:02,363: <INFO> Epoch 32, loss 0.6315
2021-05-26 21:31:02,363: <INFO> Epoch 33, loss 0.5929
2021-05-26 21:31:02,363: <INFO> Epoch 34, loss 0.6299
2021-05-26 21:31:02,364: <INFO> Epoch 35, loss 0.5749
2021-05-26 21:31:02,365: <INFO> Epoch 36, loss 0.5524
2021-05-26 21:31:02,365: <INFO> Epoch 37, loss 0.5151
2021-05-26 21:31:02,365: <INFO> Epoch 38, loss 0.5024
2021-05-26 21:31:02,366: <INFO> Epoch 39, loss 0.5004
2021-05-26 21:31:02,366: <INFO> Epoch 40, loss 0.4851
2021-05-26 21:31:02,366: <INFO> Epoch 41, loss 0.4534
2021-05-26 21:31:02,367: <INFO> Epoch 42, loss 0.4277
2021-05-26 21:31:02,367: <INFO> Epoch 43, loss 0.4644
2021-05-26 21:31:02,367: <INFO> Epoch 44, loss 0.4862
2021-05-26 21:31:02,367: <INFO> Epoch 45, loss 0.4311
2021-05-26 21:31:02,368: <INFO> Epoch 46, loss 0.3806
2021-05-26 21:31:02,368: <INFO> Epoch 47, loss 0.4026
2021-05-26 21:31:02,368: <INFO> Epoch 48, loss 0.4370
2021-05-26 21:31:02,369: <INFO> Epoch 49, loss 0.3946
2021-05-26 21:31:02,369: <INFO> Epoch 50, loss 0.3583
2021-05-26 21:31:02,369: <INFO> Epoch 51, loss 0.3412
2021-05-26 21:31:02,370: <INFO> Epoch 52, loss 0.3177
2021-05-26 21:31:02,370: <INFO> Epoch 53, loss 0.3566
2021-05-26 21:31:02,371: <INFO> Epoch 54, loss 0.3584
2021-05-26 21:31:02,372: <INFO> Epoch 55, loss 0.3363
2021-05-26 21:31:02,372: <INFO> Epoch 56, loss 0.2692
2021-05-26 21:31:02,373: <INFO> Epoch 57, loss 0.2523
2021-05-26 21:31:02,373: <INFO> Epoch 58, loss 0.2734
2021-05-26 21:31:02,374: <INFO> Epoch 59, loss 0.2879
2021-05-26 21:31:02,374: <INFO> Epoch 60, loss 0.2634
2021-05-26 21:31:02,374: <INFO> Epoch 61, loss 0.2218
2021-05-26 21:31:02,375: <INFO> Epoch 62, loss 0.2343
2021-05-26 21:31:02,375: <INFO> Epoch 63, loss 0.2405
2021-05-26 21:31:02,375: <INFO> Epoch 64, loss 0.2147
2021-05-26 21:31:02,376: <INFO> Epoch 65, loss 0.1879
2021-05-26 21:31:02,376: <INFO> Epoch 66, loss 0.2126
2021-05-26 21:31:02,376: <INFO> Epoch 67, loss 0.1855
2021-05-26 21:31:02,377: <INFO> Epoch 68, loss 0.1736
2021-05-26 21:31:02,377: <INFO> Epoch 69, loss 0.1855
2021-05-26 21:31:02,377: <INFO> Epoch 70, loss 0.1704
2021-05-26 21:31:02,378: <INFO> Epoch 71, loss 0.1680
2021-05-26 21:31:02,378: <INFO> Epoch 72, loss 0.1652
2021-05-26 21:31:02,378: <INFO> Epoch 73, loss 0.1674
2021-05-26 21:31:02,379: <INFO> Epoch 74, loss 0.1524
2021-05-26 21:31:02,379: <INFO> Epoch 75, loss 0.1632
2021-05-26 21:31:02,380: <INFO> Epoch 76, loss 0.1932
2021-05-26 21:31:02,380: <INFO> Epoch 77, loss 0.2256
2021-05-26 21:31:02,381: <INFO> Epoch 78, loss 0.2186
2021-05-26 21:31:02,382: <INFO> Epoch 79, loss 0.2557
2021-05-26 21:31:02,382: <INFO> Epoch 80, loss 0.2738
2021-05-26 21:31:02,383: <INFO> Epoch 81, loss 0.3121
2021-05-26 21:31:02,383: <INFO> Epoch 82, loss 0.2736
2021-05-26 21:31:02,384: <INFO> Epoch 83, loss 0.2642
2021-05-26 21:31:02,384: <INFO> Epoch 84, loss 0.2025
2021-05-26 21:31:02,385: <INFO> Epoch 85, loss 0.2044
2021-05-26 21:31:02,386: <INFO> Epoch 86, loss 0.1886
2021-05-26 21:31:02,386: <INFO> Epoch 87, loss 0.2366
2021-05-26 21:31:02,387: <INFO> Epoch 88, loss 0.2053
2021-05-26 21:31:02,388: <INFO> Epoch 89, loss 0.1844
2021-05-26 21:31:02,388: <INFO> Epoch 90, loss 0.1718
2021-05-26 21:31:02,388: <INFO> Epoch 91, loss 0.1833
2021-05-26 21:31:02,389: <INFO> Epoch 92, loss 0.2437
2021-05-26 21:31:02,389: <INFO> Epoch 93, loss 0.2566
2021-05-26 21:31:02,390: <INFO> Epoch 94, loss 0.2373
2021-05-26 21:31:02,391: <INFO> Epoch 95, loss 0.3685
2021-05-26 21:31:02,392: <INFO> Epoch 96, loss 0.5041
2021-05-26 21:31:02,393: <INFO> Epoch 97, loss 0.5319
2021-05-26 21:31:02,393: <INFO> Epoch 98, loss 0.4774
2021-05-26 21:31:02,394: <INFO> Epoch 99, loss 0.6959
2021-05-26 21:31:02,394: <INFO> Training model: attempt 2
2021-05-26 21:31:02,404: <WARNING> Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.
2021-05-26 21:31:02,437: <INFO> Model: "efficient"
2021-05-26 21:31:02,440: <INFO> _________________________________________________________________
2021-05-26 21:31:02,440: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-26 21:31:02,441: <INFO> =================================================================
2021-05-26 21:31:02,442: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-26 21:31:02,443: <INFO> _________________________________________________________________
2021-05-26 21:31:02,444: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-26 21:31:02,444: <INFO> _________________________________________________________________
2021-05-26 21:31:02,445: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-26 21:31:02,446: <INFO> =================================================================
2021-05-26 21:31:02,447: <INFO> Total params: 12,810
2021-05-26 21:31:02,447: <INFO> Trainable params: 12,810
2021-05-26 21:31:02,448: <INFO> Non-trainable params: 0
2021-05-26 21:31:02,448: <INFO> _________________________________________________________________
2021-05-26 21:31:31,328: <INFO> Epoch 0, loss 1.7713
2021-05-26 21:31:31,328: <INFO> Epoch 1, loss 1.4577
2021-05-26 21:31:31,329: <INFO> Epoch 2, loss 1.4229
2021-05-26 21:31:31,329: <INFO> Epoch 3, loss 1.3557
2021-05-26 21:31:31,330: <INFO> Epoch 4, loss 1.3929
2021-05-26 21:31:31,330: <INFO> Epoch 5, loss 1.7620
2021-05-26 21:31:31,331: <INFO> Epoch 6, loss 2.0455
2021-05-26 21:31:31,331: <INFO> Epoch 7, loss 2.0542
2021-05-26 21:31:31,332: <INFO> Epoch 8, loss 1.7123
2021-05-26 21:31:31,332: <INFO> Epoch 9, loss 1.4687
2021-05-26 21:31:31,333: <INFO> Epoch 10, loss 1.3413
2021-05-26 21:31:31,333: <INFO> Epoch 11, loss 1.2467
2021-05-26 21:31:31,334: <INFO> Epoch 12, loss 1.3890
2021-05-26 21:31:31,334: <INFO> Epoch 13, loss 1.4699
2021-05-26 21:31:31,335: <INFO> Epoch 14, loss 1.2437
2021-05-26 21:31:31,335: <INFO> Epoch 15, loss 1.0598
2021-05-26 21:31:31,335: <INFO> Epoch 16, loss 1.0422
2021-05-26 21:31:31,338: <INFO> Epoch 17, loss 1.1649
2021-05-26 21:31:31,339: <INFO> Epoch 18, loss 1.1012
2021-05-26 21:31:31,339: <INFO> Epoch 19, loss 0.8223
2021-05-26 21:31:31,339: <INFO> Epoch 20, loss 0.7684
2021-05-26 21:31:31,340: <INFO> Epoch 21, loss 0.7653
2021-05-26 21:31:31,340: <INFO> Epoch 22, loss 0.8165
2021-05-26 21:31:31,341: <INFO> Epoch 23, loss 0.6591
2021-05-26 21:31:31,341: <INFO> Epoch 24, loss 0.6183
2021-05-26 21:31:31,341: <INFO> Epoch 25, loss 0.6226
2021-05-26 21:31:31,341: <INFO> Epoch 26, loss 0.6895
2021-05-26 21:31:31,342: <INFO> Epoch 27, loss 0.6167
2021-05-26 21:31:31,342: <INFO> Epoch 28, loss 0.6625
2021-05-26 21:31:31,343: <INFO> Epoch 29, loss 0.7057
2021-05-26 21:31:31,343: <INFO> Epoch 30, loss 0.6634
2021-05-26 21:31:31,344: <INFO> Epoch 31, loss 0.7134
2021-05-26 21:31:31,344: <INFO> Epoch 32, loss 0.7190
2021-05-26 21:31:31,344: <INFO> Epoch 33, loss 0.6865
2021-05-26 21:31:31,345: <INFO> Epoch 34, loss 0.6324
2021-05-26 21:31:31,345: <INFO> Epoch 35, loss 0.6770
2021-05-26 21:31:31,346: <INFO> Epoch 36, loss 0.6562
2021-05-26 21:31:31,346: <INFO> Epoch 37, loss 0.6127
2021-05-26 21:31:31,347: <INFO> Epoch 38, loss 0.5445
2021-05-26 21:31:31,347: <INFO> Epoch 39, loss 0.4944
2021-05-26 21:31:31,347: <INFO> Epoch 40, loss 0.4698
2021-05-26 21:31:31,348: <INFO> Epoch 41, loss 0.4798
2021-05-26 21:31:31,348: <INFO> Epoch 42, loss 0.4332
2021-05-26 21:31:31,348: <INFO> Epoch 43, loss 0.4089
2021-05-26 21:31:31,349: <INFO> Epoch 44, loss 0.4539
2021-05-26 21:31:31,349: <INFO> Epoch 45, loss 0.4868
2021-05-26 21:31:31,349: <INFO> Epoch 46, loss 0.4423
2021-05-26 21:31:31,350: <INFO> Epoch 47, loss 0.4018
2021-05-26 21:31:31,350: <INFO> Epoch 48, loss 0.4214
2021-05-26 21:31:31,350: <INFO> Epoch 49, loss 0.4660
2021-05-26 21:31:31,351: <INFO> Epoch 50, loss 0.4045
2021-05-26 21:31:31,351: <INFO> Epoch 51, loss 0.3758
2021-05-26 21:31:31,351: <INFO> Epoch 52, loss 0.3619
2021-05-26 21:31:31,352: <INFO> Epoch 53, loss 0.3689
2021-05-26 21:31:31,352: <INFO> Epoch 54, loss 0.3918
2021-05-26 21:31:31,352: <INFO> Epoch 55, loss 0.3450
2021-05-26 21:31:31,352: <INFO> Epoch 56, loss 0.3330
2021-05-26 21:31:31,353: <INFO> Epoch 57, loss 0.3164
2021-05-26 21:31:31,354: <INFO> Epoch 58, loss 0.3469
2021-05-26 21:31:31,355: <INFO> Epoch 59, loss 0.3699
2021-05-26 21:31:31,356: <INFO> Epoch 60, loss 0.3412
2021-05-26 21:31:31,356: <INFO> Epoch 61, loss 0.2598
2021-05-26 21:31:31,357: <INFO> Epoch 62, loss 0.2255
2021-05-26 21:31:31,357: <INFO> Epoch 63, loss 0.2472
2021-05-26 21:31:31,357: <INFO> Epoch 64, loss 0.2469
2021-05-26 21:31:31,358: <INFO> Epoch 65, loss 0.2414
2021-05-26 21:31:31,359: <INFO> Epoch 66, loss 0.2155
2021-05-26 21:31:31,360: <INFO> Epoch 67, loss 0.2210
2021-05-26 21:31:31,360: <INFO> Epoch 68, loss 0.1937
2021-05-26 21:31:31,361: <INFO> Epoch 69, loss 0.1740
2021-05-26 21:31:31,362: <INFO> Epoch 70, loss 0.1832
2021-05-26 21:31:31,362: <INFO> Epoch 71, loss 0.1780
2021-05-26 21:31:31,363: <INFO> Epoch 72, loss 0.1860
2021-05-26 21:31:31,363: <INFO> Epoch 73, loss 0.1886
2021-05-26 21:31:31,364: <INFO> Epoch 74, loss 0.1859
2021-05-26 21:31:31,365: <INFO> Epoch 75, loss 0.1639
2021-05-26 21:31:31,365: <INFO> Epoch 76, loss 0.1724
2021-05-26 21:31:31,365: <INFO> Epoch 77, loss 0.1912
2021-05-26 21:31:31,366: <INFO> Epoch 78, loss 0.2138
2021-05-26 21:31:31,366: <INFO> Epoch 79, loss 0.2016
2021-05-26 21:31:31,366: <INFO> Epoch 80, loss 0.2328
2021-05-26 21:31:31,366: <INFO> Epoch 81, loss 0.2527
2021-05-26 21:31:31,367: <INFO> Epoch 82, loss 0.2939
2021-05-26 21:31:31,368: <INFO> Epoch 83, loss 0.2810
2021-05-26 21:31:31,369: <INFO> Epoch 84, loss 0.2771
2021-05-26 21:31:31,369: <INFO> Epoch 85, loss 0.2714
2021-05-26 21:31:31,370: <INFO> Epoch 86, loss 0.2327
2021-05-26 21:31:31,371: <INFO> Epoch 87, loss 0.2598
2021-05-26 21:31:31,371: <INFO> Epoch 88, loss 0.3231
2021-05-26 21:31:31,372: <INFO> Epoch 89, loss 0.3388
2021-05-26 21:31:31,373: <INFO> Epoch 90, loss 0.3431
2021-05-26 21:31:31,373: <INFO> Epoch 91, loss 0.3958
2021-05-26 21:31:31,374: <INFO> Epoch 92, loss 0.4950
2021-05-26 21:31:31,374: <INFO> Epoch 93, loss 0.6615
2021-05-26 21:31:31,375: <INFO> Epoch 94, loss 0.6175
2021-05-26 21:31:31,375: <INFO> Epoch 95, loss 0.8320
2021-05-26 21:31:31,376: <INFO> Epoch 96, loss 0.9830
2021-05-26 21:31:31,376: <INFO> Epoch 97, loss 1.1829
2021-05-26 21:31:31,376: <INFO> Epoch 98, loss 0.8084
2021-05-26 21:31:31,377: <INFO> Epoch 99, loss 0.7406
2021-05-26 21:31:32,996: <INFO> Assets written to: trained_models/efficent\assets
2021-05-26 21:31:33,087: <INFO> Best loss is 0.695871
