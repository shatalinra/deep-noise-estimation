2021-05-25 22:20:14,287: <INFO> Trying efficent model
2021-05-25 22:20:52,021: <INFO> Training data size 900
2021-05-25 22:21:53,511: <INFO> Training model: attempt 0
2021-05-25 22:21:53,983: <INFO> Model: "efficient"
2021-05-25 22:21:53,983: <INFO> _________________________________________________________________
2021-05-25 22:21:53,984: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-25 22:21:53,984: <INFO> =================================================================
2021-05-25 22:21:53,985: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-25 22:21:53,986: <INFO> _________________________________________________________________
2021-05-25 22:21:53,987: <INFO> dropout (Dropout)            (None, 1280)              0         
2021-05-25 22:21:53,987: <INFO> _________________________________________________________________
2021-05-25 22:21:53,988: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-25 22:21:53,989: <INFO> _________________________________________________________________
2021-05-25 22:21:53,990: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-25 22:21:53,990: <INFO> =================================================================
2021-05-25 22:21:53,991: <INFO> Total params: 12,810
2021-05-25 22:21:53,992: <INFO> Trainable params: 12,810
2021-05-25 22:21:53,992: <INFO> Non-trainable params: 0
2021-05-25 22:21:53,994: <INFO> _________________________________________________________________
2021-05-25 22:22:17,986: <INFO> Trying efficent model
2021-05-25 22:22:53,800: <INFO> Training data size 900
2021-05-25 22:23:57,796: <INFO> Training model: attempt 0
2021-05-25 22:23:58,242: <INFO> Model: "efficient"
2021-05-25 22:23:58,242: <INFO> _________________________________________________________________
2021-05-25 22:23:58,242: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-25 22:23:58,243: <INFO> =================================================================
2021-05-25 22:23:58,244: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-25 22:23:58,245: <INFO> _________________________________________________________________
2021-05-25 22:23:58,246: <INFO> dropout (Dropout)            (None, 1280)              0         
2021-05-25 22:23:58,246: <INFO> _________________________________________________________________
2021-05-25 22:23:58,247: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-25 22:23:58,247: <INFO> _________________________________________________________________
2021-05-25 22:23:58,248: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-25 22:23:58,249: <INFO> =================================================================
2021-05-25 22:23:58,250: <INFO> Total params: 12,810
2021-05-25 22:23:58,251: <INFO> Trainable params: 12,810
2021-05-25 22:23:58,252: <INFO> Non-trainable params: 0
2021-05-25 22:23:58,252: <INFO> _________________________________________________________________
2021-05-25 22:24:46,144: <INFO> Trying efficent model
2021-05-25 22:25:21,186: <INFO> Training data size 900
2021-05-25 22:26:21,958: <INFO> Training model: attempt 0
2021-05-25 22:26:22,406: <INFO> Model: "efficient"
2021-05-25 22:26:22,406: <INFO> _________________________________________________________________
2021-05-25 22:26:22,406: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-25 22:26:22,407: <INFO> =================================================================
2021-05-25 22:26:22,408: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-25 22:26:22,409: <INFO> _________________________________________________________________
2021-05-25 22:26:22,409: <INFO> dropout (Dropout)            (None, 1280)              0         
2021-05-25 22:26:22,410: <INFO> _________________________________________________________________
2021-05-25 22:26:22,411: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-25 22:26:22,411: <INFO> _________________________________________________________________
2021-05-25 22:26:22,412: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-25 22:26:22,412: <INFO> =================================================================
2021-05-25 22:26:22,414: <INFO> Total params: 12,810
2021-05-25 22:26:22,414: <INFO> Trainable params: 12,810
2021-05-25 22:26:22,415: <INFO> Non-trainable params: 0
2021-05-25 22:26:22,415: <INFO> _________________________________________________________________
2021-05-25 22:27:32,450: <INFO> Epoch 0, loss 2.2265
2021-05-25 22:27:32,450: <INFO> Epoch 1, loss 1.8443
2021-05-25 22:27:32,451: <INFO> Epoch 2, loss 1.7113
2021-05-25 22:27:32,451: <INFO> Epoch 3, loss 1.6178
2021-05-25 22:27:32,452: <INFO> Epoch 4, loss 1.5487
2021-05-25 22:27:32,452: <INFO> Epoch 5, loss 1.5272
2021-05-25 22:27:32,453: <INFO> Epoch 6, loss 1.4624
2021-05-25 22:27:32,454: <INFO> Epoch 7, loss 1.4180
2021-05-25 22:27:32,454: <INFO> Epoch 8, loss 1.3788
2021-05-25 22:27:32,455: <INFO> Epoch 9, loss 1.3410
2021-05-25 22:27:32,455: <INFO> Epoch 10, loss 1.3396
2021-05-25 22:27:32,456: <INFO> Epoch 11, loss 1.3181
2021-05-25 22:27:32,456: <INFO> Epoch 12, loss 1.3108
2021-05-25 22:27:32,457: <INFO> Epoch 13, loss 1.2729
2021-05-25 22:27:32,457: <INFO> Epoch 14, loss 1.2428
2021-05-25 22:27:32,458: <INFO> Epoch 15, loss 1.2352
2021-05-25 22:27:32,458: <INFO> Epoch 16, loss 1.2079
2021-05-25 22:27:32,458: <INFO> Epoch 17, loss 1.1984
2021-05-25 22:27:32,458: <INFO> Epoch 18, loss 1.2030
2021-05-25 22:27:32,459: <INFO> Epoch 19, loss 1.1643
2021-05-25 22:27:32,459: <INFO> Epoch 20, loss 1.1640
2021-05-25 22:27:32,459: <INFO> Epoch 21, loss 1.1608
2021-05-25 22:27:32,460: <INFO> Epoch 22, loss 1.1549
2021-05-25 22:27:32,461: <INFO> Epoch 23, loss 1.1399
2021-05-25 22:27:32,461: <INFO> Epoch 24, loss 1.1376
2021-05-25 22:27:32,461: <INFO> Epoch 25, loss 1.1356
2021-05-25 22:27:32,462: <INFO> Epoch 26, loss 1.1006
2021-05-25 22:27:32,462: <INFO> Epoch 27, loss 1.0837
2021-05-25 22:27:32,463: <INFO> Epoch 28, loss 1.0891
2021-05-25 22:27:32,463: <INFO> Epoch 29, loss 1.0943
2021-05-25 22:27:32,464: <INFO> Epoch 30, loss 1.0807
2021-05-25 22:27:32,464: <INFO> Epoch 31, loss 1.0550
2021-05-25 22:27:32,464: <INFO> Epoch 32, loss 1.0549
2021-05-25 22:27:32,465: <INFO> Epoch 33, loss 1.0512
2021-05-25 22:27:32,465: <INFO> Epoch 34, loss 1.0464
2021-05-25 22:27:32,465: <INFO> Epoch 35, loss 1.0415
2021-05-25 22:27:32,466: <INFO> Epoch 36, loss 1.0245
2021-05-25 22:27:32,466: <INFO> Epoch 37, loss 1.0284
2021-05-25 22:27:32,466: <INFO> Epoch 38, loss 1.0234
2021-05-25 22:27:32,467: <INFO> Epoch 39, loss 1.0081
2021-05-25 22:27:32,467: <INFO> Epoch 40, loss 1.0256
2021-05-25 22:27:32,468: <INFO> Epoch 41, loss 0.9898
2021-05-25 22:27:32,468: <INFO> Epoch 42, loss 0.9982
2021-05-25 22:27:32,469: <INFO> Epoch 43, loss 0.9959
2021-05-25 22:27:32,470: <INFO> Epoch 44, loss 0.9738
2021-05-25 22:27:32,471: <INFO> Epoch 45, loss 0.9993
2021-05-25 22:27:32,471: <INFO> Epoch 46, loss 0.9574
2021-05-25 22:27:32,472: <INFO> Epoch 47, loss 0.9968
2021-05-25 22:27:32,472: <INFO> Epoch 48, loss 0.9833
2021-05-25 22:27:32,472: <INFO> Epoch 49, loss 0.9656
2021-05-25 22:27:32,473: <INFO> Epoch 50, loss 0.9645
2021-05-25 22:27:32,474: <INFO> Epoch 51, loss 0.9527
2021-05-25 22:27:32,474: <INFO> Epoch 52, loss 0.9515
2021-05-25 22:27:32,474: <INFO> Epoch 53, loss 0.9390
2021-05-25 22:27:32,475: <INFO> Epoch 54, loss 0.9432
2021-05-25 22:27:32,476: <INFO> Epoch 55, loss 0.9499
2021-05-25 22:27:32,476: <INFO> Epoch 56, loss 0.9285
2021-05-25 22:27:32,476: <INFO> Epoch 57, loss 0.9281
2021-05-25 22:27:32,478: <INFO> Epoch 58, loss 0.9192
2021-05-25 22:27:32,479: <INFO> Epoch 59, loss 0.8987
2021-05-25 22:27:32,479: <INFO> Epoch 60, loss 0.9095
2021-05-25 22:27:32,480: <INFO> Epoch 61, loss 0.9263
2021-05-25 22:27:32,481: <INFO> Epoch 62, loss 0.9035
2021-05-25 22:27:32,481: <INFO> Epoch 63, loss 0.9153
2021-05-25 22:27:32,482: <INFO> Epoch 64, loss 0.8955
2021-05-25 22:27:32,483: <INFO> Epoch 65, loss 0.9087
2021-05-25 22:27:32,484: <INFO> Epoch 66, loss 0.8782
2021-05-25 22:27:32,485: <INFO> Epoch 67, loss 0.8812
2021-05-25 22:27:32,486: <INFO> Epoch 68, loss 0.8870
2021-05-25 22:27:32,486: <INFO> Epoch 69, loss 0.8807
2021-05-25 22:27:32,487: <INFO> Epoch 70, loss 0.8786
2021-05-25 22:27:32,488: <INFO> Epoch 71, loss 0.8924
2021-05-25 22:27:32,488: <INFO> Epoch 72, loss 0.8754
2021-05-25 22:27:32,489: <INFO> Epoch 73, loss 0.8889
2021-05-25 22:27:32,490: <INFO> Epoch 74, loss 0.8577
2021-05-25 22:27:32,490: <INFO> Epoch 75, loss 0.8659
2021-05-25 22:27:32,491: <INFO> Epoch 76, loss 0.8783
2021-05-25 22:27:32,491: <INFO> Epoch 77, loss 0.8720
2021-05-25 22:27:32,491: <INFO> Epoch 78, loss 0.8468
2021-05-25 22:27:32,492: <INFO> Epoch 79, loss 0.8510
2021-05-25 22:27:32,493: <INFO> Epoch 80, loss 0.8482
2021-05-25 22:27:32,493: <INFO> Epoch 81, loss 0.8633
2021-05-25 22:27:32,494: <INFO> Epoch 82, loss 0.8473
2021-05-25 22:27:32,495: <INFO> Epoch 83, loss 0.8463
2021-05-25 22:27:32,495: <INFO> Epoch 84, loss 0.8367
2021-05-25 22:27:32,496: <INFO> Epoch 85, loss 0.8347
2021-05-25 22:27:32,496: <INFO> Epoch 86, loss 0.8627
2021-05-25 22:27:32,497: <INFO> Epoch 87, loss 0.8350
2021-05-25 22:27:32,498: <INFO> Epoch 88, loss 0.8476
2021-05-25 22:27:32,498: <INFO> Epoch 89, loss 0.8483
2021-05-25 22:27:32,499: <INFO> Epoch 90, loss 0.8488
2021-05-25 22:27:32,499: <INFO> Epoch 91, loss 0.8311
2021-05-25 22:27:32,500: <INFO> Epoch 92, loss 0.8315
2021-05-25 22:27:32,500: <INFO> Epoch 93, loss 0.8115
2021-05-25 22:27:32,501: <INFO> Epoch 94, loss 0.8223
2021-05-25 22:27:32,501: <INFO> Epoch 95, loss 0.8373
2021-05-25 22:27:32,502: <INFO> Epoch 96, loss 0.8177
2021-05-25 22:27:32,502: <INFO> Epoch 97, loss 0.8493
2021-05-25 22:27:32,502: <INFO> Epoch 98, loss 0.8104
2021-05-25 22:27:32,505: <INFO> Epoch 99, loss 0.8128
2021-05-25 22:27:32,506: <INFO> Epoch 100, loss 0.8195
2021-05-25 22:27:32,507: <INFO> Epoch 101, loss 0.7996
2021-05-25 22:27:32,507: <INFO> Epoch 102, loss 0.8053
2021-05-25 22:27:32,508: <INFO> Epoch 103, loss 0.8197
2021-05-25 22:27:32,509: <INFO> Epoch 104, loss 0.8153
2021-05-25 22:27:32,509: <INFO> Epoch 105, loss 0.8078
2021-05-25 22:27:32,510: <INFO> Epoch 106, loss 0.7966
2021-05-25 22:27:32,510: <INFO> Epoch 107, loss 0.8069
2021-05-25 22:27:32,511: <INFO> Epoch 108, loss 0.7945
2021-05-25 22:27:32,511: <INFO> Epoch 109, loss 0.7965
2021-05-25 22:27:32,512: <INFO> Epoch 110, loss 0.7787
2021-05-25 22:27:32,513: <INFO> Epoch 111, loss 0.8243
2021-05-25 22:27:32,514: <INFO> Epoch 112, loss 0.8070
2021-05-25 22:27:32,514: <INFO> Epoch 113, loss 0.7895
2021-05-25 22:27:32,515: <INFO> Epoch 114, loss 0.7931
2021-05-25 22:27:32,515: <INFO> Epoch 115, loss 0.7574
2021-05-25 22:27:32,516: <INFO> Epoch 116, loss 0.7851
2021-05-25 22:27:32,516: <INFO> Epoch 117, loss 0.8129
2021-05-25 22:27:32,517: <INFO> Epoch 118, loss 0.7732
2021-05-25 22:27:32,518: <INFO> Epoch 119, loss 0.7794
2021-05-25 22:27:32,518: <INFO> Epoch 120, loss 0.7959
2021-05-25 22:27:32,520: <INFO> Epoch 121, loss 0.7702
2021-05-25 22:27:32,520: <INFO> Epoch 122, loss 0.7795
2021-05-25 22:27:32,521: <INFO> Epoch 123, loss 0.7867
2021-05-25 22:27:32,521: <INFO> Epoch 124, loss 0.7706
2021-05-25 22:27:32,522: <INFO> Epoch 125, loss 0.7583
2021-05-25 22:27:32,522: <INFO> Epoch 126, loss 0.7974
2021-05-25 22:27:32,522: <INFO> Epoch 127, loss 0.7935
2021-05-25 22:27:32,523: <INFO> Epoch 128, loss 0.7592
2021-05-25 22:27:32,524: <INFO> Epoch 129, loss 0.7680
2021-05-25 22:27:32,524: <INFO> Epoch 130, loss 0.8033
2021-05-25 22:27:32,525: <INFO> Epoch 131, loss 0.7804
2021-05-25 22:27:32,526: <INFO> Epoch 132, loss 0.7363
2021-05-25 22:27:32,526: <INFO> Epoch 133, loss 0.7655
2021-05-25 22:27:32,527: <INFO> Epoch 134, loss 0.7637
2021-05-25 22:27:32,528: <INFO> Epoch 135, loss 0.7878
2021-05-25 22:27:32,528: <INFO> Epoch 136, loss 0.7540
2021-05-25 22:27:32,529: <INFO> Epoch 137, loss 0.7616
2021-05-25 22:27:32,529: <INFO> Epoch 138, loss 0.7573
2021-05-25 22:27:32,530: <INFO> Epoch 139, loss 0.7605
2021-05-25 22:27:32,530: <INFO> Epoch 140, loss 0.7466
2021-05-25 22:27:32,531: <INFO> Epoch 141, loss 0.7697
2021-05-25 22:27:32,531: <INFO> Epoch 142, loss 0.7622
2021-05-25 22:27:32,532: <INFO> Epoch 143, loss 0.7620
2021-05-25 22:27:32,532: <INFO> Epoch 144, loss 0.7560
2021-05-25 22:27:32,532: <INFO> Epoch 145, loss 0.7548
2021-05-25 22:27:32,533: <INFO> Epoch 146, loss 0.7310
2021-05-25 22:27:32,534: <INFO> Epoch 147, loss 0.7490
2021-05-25 22:27:32,535: <INFO> Epoch 148, loss 0.7359
2021-05-25 22:27:32,535: <INFO> Epoch 149, loss 0.7573
2021-05-25 22:27:32,537: <INFO> Epoch 150, loss 0.7606
2021-05-25 22:27:32,538: <INFO> Epoch 151, loss 0.7817
2021-05-25 22:27:32,538: <INFO> Epoch 152, loss 0.7468
2021-05-25 22:27:32,539: <INFO> Epoch 153, loss 0.7372
2021-05-25 22:27:32,540: <INFO> Epoch 154, loss 0.7393
2021-05-25 22:27:32,540: <INFO> Epoch 155, loss 0.7446
2021-05-25 22:27:32,541: <INFO> Epoch 156, loss 0.7519
2021-05-25 22:27:32,541: <INFO> Epoch 157, loss 0.7429
2021-05-25 22:27:32,542: <INFO> Epoch 158, loss 0.7269
2021-05-25 22:27:32,542: <INFO> Epoch 159, loss 0.7333
2021-05-25 22:27:32,542: <INFO> Epoch 160, loss 0.7293
2021-05-25 22:27:32,543: <INFO> Epoch 161, loss 0.7384
2021-05-25 22:27:32,543: <INFO> Epoch 162, loss 0.7643
2021-05-25 22:27:32,544: <INFO> Epoch 163, loss 0.7036
2021-05-25 22:27:32,545: <INFO> Epoch 164, loss 0.7307
2021-05-25 22:27:32,546: <INFO> Epoch 165, loss 0.7538
2021-05-25 22:27:32,546: <INFO> Epoch 166, loss 0.7348
2021-05-25 22:27:32,547: <INFO> Epoch 167, loss 0.7293
2021-05-25 22:27:32,547: <INFO> Epoch 168, loss 0.7356
2021-05-25 22:27:32,548: <INFO> Epoch 169, loss 0.7346
2021-05-25 22:27:32,548: <INFO> Epoch 170, loss 0.7556
2021-05-25 22:27:32,549: <INFO> Epoch 171, loss 0.7063
2021-05-25 22:27:32,550: <INFO> Epoch 172, loss 0.7197
2021-05-25 22:27:32,550: <INFO> Epoch 173, loss 0.7035
2021-05-25 22:27:32,551: <INFO> Epoch 174, loss 0.7305
2021-05-25 22:27:32,551: <INFO> Epoch 175, loss 0.6908
2021-05-25 22:27:32,552: <INFO> Epoch 176, loss 0.7188
2021-05-25 22:27:32,552: <INFO> Epoch 177, loss 0.6914
2021-05-25 22:27:32,553: <INFO> Epoch 178, loss 0.7208
2021-05-25 22:27:32,555: <INFO> Epoch 179, loss 0.7044
2021-05-25 22:27:32,556: <INFO> Epoch 180, loss 0.7037
2021-05-25 22:27:32,557: <INFO> Epoch 181, loss 0.7221
2021-05-25 22:27:32,557: <INFO> Epoch 182, loss 0.7061
2021-05-25 22:27:32,558: <INFO> Epoch 183, loss 0.7102
2021-05-25 22:27:32,559: <INFO> Epoch 184, loss 0.7114
2021-05-25 22:27:32,559: <INFO> Epoch 185, loss 0.7073
2021-05-25 22:27:32,560: <INFO> Epoch 186, loss 0.7193
2021-05-25 22:27:32,560: <INFO> Epoch 187, loss 0.7052
2021-05-25 22:27:32,561: <INFO> Epoch 188, loss 0.7217
2021-05-25 22:27:32,561: <INFO> Epoch 189, loss 0.7112
2021-05-25 22:27:32,561: <INFO> Epoch 190, loss 0.7062
2021-05-25 22:27:32,562: <INFO> Epoch 191, loss 0.7226
2021-05-25 22:27:32,562: <INFO> Epoch 192, loss 0.7418
2021-05-25 22:27:32,563: <INFO> Epoch 193, loss 0.7224
2021-05-25 22:27:32,564: <INFO> Epoch 194, loss 0.7131
2021-05-25 22:27:32,564: <INFO> Epoch 195, loss 0.7001
2021-05-25 22:27:32,565: <INFO> Epoch 196, loss 0.7076
2021-05-25 22:27:32,566: <INFO> Epoch 197, loss 0.6969
2021-05-25 22:27:32,566: <INFO> Epoch 198, loss 0.6904
2021-05-25 22:27:32,567: <INFO> Epoch 199, loss 0.7254
2021-05-25 22:27:32,567: <INFO> Epoch 200, loss 0.7418
2021-05-25 22:27:32,568: <INFO> Epoch 201, loss 0.6999
2021-05-25 22:27:32,568: <INFO> Epoch 202, loss 0.7109
2021-05-25 22:27:32,570: <INFO> Epoch 203, loss 0.7124
2021-05-25 22:27:32,571: <INFO> Epoch 204, loss 0.7320
2021-05-25 22:27:32,572: <INFO> Epoch 205, loss 0.7013
2021-05-25 22:27:32,574: <INFO> Epoch 206, loss 0.6875
2021-05-25 22:27:32,574: <INFO> Epoch 207, loss 0.6856
2021-05-25 22:27:32,575: <INFO> Epoch 208, loss 0.7104
2021-05-25 22:27:32,575: <INFO> Epoch 209, loss 0.7021
2021-05-25 22:27:32,576: <INFO> Epoch 210, loss 0.6958
2021-05-25 22:27:32,577: <INFO> Epoch 211, loss 0.7004
2021-05-25 22:27:32,577: <INFO> Epoch 212, loss 0.6898
2021-05-25 22:27:32,578: <INFO> Epoch 213, loss 0.6952
2021-05-25 22:27:32,578: <INFO> Epoch 214, loss 0.7018
2021-05-25 22:27:32,579: <INFO> Epoch 215, loss 0.7042
2021-05-25 22:27:32,580: <INFO> Epoch 216, loss 0.6933
2021-05-25 22:27:32,580: <INFO> Epoch 217, loss 0.6938
2021-05-25 22:27:32,581: <INFO> Epoch 218, loss 0.6881
2021-05-25 22:27:32,581: <INFO> Epoch 219, loss 0.6977
2021-05-25 22:27:32,581: <INFO> Epoch 220, loss 0.6827
2021-05-25 22:27:32,582: <INFO> Epoch 221, loss 0.6806
2021-05-25 22:27:32,582: <INFO> Epoch 222, loss 0.7069
2021-05-25 22:27:32,582: <INFO> Epoch 223, loss 0.6918
2021-05-25 22:27:32,584: <INFO> Epoch 224, loss 0.6892
2021-05-25 22:27:32,584: <INFO> Epoch 225, loss 0.6916
2021-05-25 22:27:32,585: <INFO> Epoch 226, loss 0.7119
2021-05-25 22:27:32,586: <INFO> Epoch 227, loss 0.7012
2021-05-25 22:27:32,587: <INFO> Epoch 228, loss 0.6802
2021-05-25 22:27:32,588: <INFO> Epoch 229, loss 0.7135
2021-05-25 22:27:32,588: <INFO> Epoch 230, loss 0.7070
2021-05-25 22:27:32,589: <INFO> Epoch 231, loss 0.7313
2021-05-25 22:27:32,589: <INFO> Epoch 232, loss 0.6805
2021-05-25 22:27:32,590: <INFO> Epoch 233, loss 0.6781
2021-05-25 22:27:32,591: <INFO> Epoch 234, loss 0.6879
2021-05-25 22:27:32,591: <INFO> Epoch 235, loss 0.6612
2021-05-25 22:27:32,592: <INFO> Epoch 236, loss 0.6601
2021-05-25 22:27:32,592: <INFO> Epoch 237, loss 0.6896
2021-05-25 22:27:32,592: <INFO> Epoch 238, loss 0.6601
2021-05-25 22:27:32,593: <INFO> Epoch 239, loss 0.6869
2021-05-25 22:27:32,594: <INFO> Epoch 240, loss 0.6720
2021-05-25 22:27:32,594: <INFO> Epoch 241, loss 0.7001
2021-05-25 22:27:32,595: <INFO> Epoch 242, loss 0.6627
2021-05-25 22:27:32,595: <INFO> Epoch 243, loss 0.6744
2021-05-25 22:27:32,596: <INFO> Epoch 244, loss 0.6994
2021-05-25 22:27:32,597: <INFO> Epoch 245, loss 0.6821
2021-05-25 22:27:32,597: <INFO> Epoch 246, loss 0.6880
2021-05-25 22:27:32,598: <INFO> Epoch 247, loss 0.6767
2021-05-25 22:27:32,598: <INFO> Epoch 248, loss 0.6842
2021-05-25 22:27:32,599: <INFO> Epoch 249, loss 0.6722
2021-05-25 22:27:32,600: <INFO> Epoch 250, loss 0.6873
2021-05-25 22:27:32,600: <INFO> Epoch 251, loss 0.6776
2021-05-25 22:27:32,601: <INFO> Epoch 252, loss 0.6638
2021-05-25 22:27:32,601: <INFO> Epoch 253, loss 0.6504
2021-05-25 22:27:32,601: <INFO> Epoch 254, loss 0.6925
2021-05-25 22:27:32,602: <INFO> Epoch 255, loss 0.6848
2021-05-25 22:27:32,603: <INFO> Epoch 256, loss 0.6649
2021-05-25 22:27:32,603: <INFO> Epoch 257, loss 0.6968
2021-05-25 22:27:32,605: <INFO> Epoch 258, loss 0.6359
2021-05-25 22:27:32,605: <INFO> Epoch 259, loss 0.6934
2021-05-25 22:27:32,606: <INFO> Epoch 260, loss 0.6588
2021-05-25 22:27:32,607: <INFO> Epoch 261, loss 0.6656
2021-05-25 22:27:32,608: <INFO> Epoch 262, loss 0.6856
2021-05-25 22:27:32,608: <INFO> Epoch 263, loss 0.6722
2021-05-25 22:27:32,609: <INFO> Epoch 264, loss 0.6561
2021-05-25 22:27:32,609: <INFO> Epoch 265, loss 0.7039
2021-05-25 22:27:32,610: <INFO> Epoch 266, loss 0.6509
2021-05-25 22:27:32,611: <INFO> Epoch 267, loss 0.6678
2021-05-25 22:27:32,611: <INFO> Epoch 268, loss 0.6594
2021-05-25 22:27:32,612: <INFO> Epoch 269, loss 0.6873
2021-05-25 22:27:32,612: <INFO> Epoch 270, loss 0.6752
2021-05-25 22:27:32,612: <INFO> Epoch 271, loss 0.6843
2021-05-25 22:27:32,613: <INFO> Epoch 272, loss 0.6756
2021-05-25 22:27:32,613: <INFO> Epoch 273, loss 0.6320
2021-05-25 22:27:32,614: <INFO> Epoch 274, loss 0.6902
2021-05-25 22:27:32,614: <INFO> Epoch 275, loss 0.6512
2021-05-25 22:27:32,615: <INFO> Epoch 276, loss 0.6686
2021-05-25 22:27:32,615: <INFO> Epoch 277, loss 0.6497
2021-05-25 22:27:32,615: <INFO> Epoch 278, loss 0.6671
2021-05-25 22:27:32,616: <INFO> Epoch 279, loss 0.6639
2021-05-25 22:27:32,616: <INFO> Epoch 280, loss 0.6519
2021-05-25 22:27:32,616: <INFO> Epoch 281, loss 0.6585
2021-05-25 22:27:32,617: <INFO> Epoch 282, loss 0.6562
2021-05-25 22:27:32,618: <INFO> Epoch 283, loss 0.6649
2021-05-25 22:27:32,618: <INFO> Epoch 284, loss 0.6450
2021-05-25 22:27:32,619: <INFO> Epoch 285, loss 0.6720
2021-05-25 22:27:32,619: <INFO> Epoch 286, loss 0.6561
2021-05-25 22:27:32,619: <INFO> Epoch 287, loss 0.6461
2021-05-25 22:27:32,620: <INFO> Epoch 288, loss 0.6596
2021-05-25 22:27:32,621: <INFO> Epoch 289, loss 0.6657
2021-05-25 22:27:32,622: <INFO> Epoch 290, loss 0.6573
2021-05-25 22:27:32,623: <INFO> Epoch 291, loss 0.6339
2021-05-25 22:27:32,624: <INFO> Epoch 292, loss 0.6643
2021-05-25 22:27:32,625: <INFO> Epoch 293, loss 0.6593
2021-05-25 22:27:32,626: <INFO> Epoch 294, loss 0.6571
2021-05-25 22:27:32,627: <INFO> Epoch 295, loss 0.6479
2021-05-25 22:27:32,627: <INFO> Epoch 296, loss 0.6610
2021-05-25 22:27:32,628: <INFO> Epoch 297, loss 0.6734
2021-05-25 22:27:32,629: <INFO> Epoch 298, loss 0.6537
2021-05-25 22:27:32,630: <INFO> Epoch 299, loss 0.6666
2021-05-25 22:27:32,630: <INFO> Epoch 300, loss 0.6481
2021-05-25 22:27:32,631: <INFO> Epoch 301, loss 0.6380
2021-05-25 22:27:32,631: <INFO> Epoch 302, loss 0.6667
2021-05-25 22:27:32,632: <INFO> Epoch 303, loss 0.6700
2021-05-25 22:27:32,632: <INFO> Epoch 304, loss 0.6361
2021-05-25 22:27:32,633: <INFO> Epoch 305, loss 0.6641
2021-05-25 22:27:32,633: <INFO> Epoch 306, loss 0.6591
2021-05-25 22:27:32,633: <INFO> Epoch 307, loss 0.6573
2021-05-25 22:27:32,634: <INFO> Epoch 308, loss 0.6550
2021-05-25 22:27:32,635: <INFO> Epoch 309, loss 0.6463
2021-05-25 22:27:32,636: <INFO> Epoch 310, loss 0.6603
2021-05-25 22:27:32,636: <INFO> Epoch 311, loss 0.6349
2021-05-25 22:27:32,637: <INFO> Epoch 312, loss 0.6385
2021-05-25 22:27:32,637: <INFO> Epoch 313, loss 0.6504
2021-05-25 22:27:32,638: <INFO> Epoch 314, loss 0.6732
2021-05-25 22:27:32,638: <INFO> Epoch 315, loss 0.6381
2021-05-25 22:27:32,639: <INFO> Epoch 316, loss 0.6390
2021-05-25 22:27:32,639: <INFO> Epoch 317, loss 0.6740
2021-05-25 22:27:32,640: <INFO> Epoch 318, loss 0.6652
2021-05-25 22:27:32,640: <INFO> Epoch 319, loss 0.6335
2021-05-25 22:27:32,641: <INFO> Epoch 320, loss 0.6266
2021-05-25 22:27:32,642: <INFO> Epoch 321, loss 0.6643
2021-05-25 22:27:32,643: <INFO> Epoch 322, loss 0.6530
2021-05-25 22:27:32,643: <INFO> Epoch 323, loss 0.6385
2021-05-25 22:27:32,644: <INFO> Epoch 324, loss 0.6258
2021-05-25 22:27:32,645: <INFO> Epoch 325, loss 0.6424
2021-05-25 22:27:32,646: <INFO> Epoch 326, loss 0.6142
2021-05-25 22:27:32,647: <INFO> Epoch 327, loss 0.6525
2021-05-25 22:27:32,647: <INFO> Epoch 328, loss 0.6393
2021-05-25 22:27:32,648: <INFO> Epoch 329, loss 0.6195
2021-05-25 22:27:32,648: <INFO> Epoch 330, loss 0.6374
2021-05-25 22:27:32,649: <INFO> Epoch 331, loss 0.6474
2021-05-25 22:27:32,650: <INFO> Epoch 332, loss 0.6513
2021-05-25 22:27:32,650: <INFO> Epoch 333, loss 0.6498
2021-05-25 22:27:32,651: <INFO> Epoch 334, loss 0.6306
2021-05-25 22:27:32,651: <INFO> Epoch 335, loss 0.6406
2021-05-25 22:27:32,652: <INFO> Epoch 336, loss 0.6346
2021-05-25 22:27:32,653: <INFO> Epoch 337, loss 0.6447
2021-05-25 22:27:32,654: <INFO> Epoch 338, loss 0.6315
2021-05-25 22:27:32,656: <INFO> Epoch 339, loss 0.6508
2021-05-25 22:27:32,656: <INFO> Epoch 340, loss 0.6437
2021-05-25 22:27:32,657: <INFO> Epoch 341, loss 0.6584
2021-05-25 22:27:32,658: <INFO> Epoch 342, loss 0.6496
2021-05-25 22:27:32,658: <INFO> Epoch 343, loss 0.6450
2021-05-25 22:27:32,659: <INFO> Epoch 344, loss 0.6639
2021-05-25 22:27:32,660: <INFO> Epoch 345, loss 0.6495
2021-05-25 22:27:32,660: <INFO> Epoch 346, loss 0.6477
2021-05-25 22:27:32,661: <INFO> Epoch 347, loss 0.6408
2021-05-25 22:27:32,661: <INFO> Epoch 348, loss 0.6360
2021-05-25 22:27:32,662: <INFO> Epoch 349, loss 0.6782
2021-05-25 22:27:32,663: <INFO> Epoch 350, loss 0.6325
2021-05-25 22:27:32,663: <INFO> Epoch 351, loss 0.6431
2021-05-25 22:27:32,664: <INFO> Epoch 352, loss 0.6189
2021-05-25 22:27:32,664: <INFO> Epoch 353, loss 0.6097
2021-05-25 22:27:32,664: <INFO> Epoch 354, loss 0.6731
2021-05-25 22:27:32,665: <INFO> Epoch 355, loss 0.6250
2021-05-25 22:27:32,666: <INFO> Epoch 356, loss 0.6330
2021-05-25 22:27:32,666: <INFO> Epoch 357, loss 0.6653
2021-05-25 22:27:32,667: <INFO> Epoch 358, loss 0.6398
2021-05-25 22:27:32,668: <INFO> Epoch 359, loss 0.6313
2021-05-25 22:27:32,668: <INFO> Epoch 360, loss 0.6417
2021-05-25 22:27:32,669: <INFO> Epoch 361, loss 0.6529
2021-05-25 22:27:32,670: <INFO> Epoch 362, loss 0.6026
2021-05-25 22:27:32,670: <INFO> Epoch 363, loss 0.6419
2021-05-25 22:27:32,671: <INFO> Epoch 364, loss 0.6428
2021-05-25 22:27:32,672: <INFO> Epoch 365, loss 0.6565
2021-05-25 22:27:32,672: <INFO> Epoch 366, loss 0.6551
2021-05-25 22:27:32,673: <INFO> Epoch 367, loss 0.6330
2021-05-25 22:27:32,673: <INFO> Epoch 368, loss 0.6322
2021-05-25 22:27:32,674: <INFO> Epoch 369, loss 0.6310
2021-05-25 22:27:32,674: <INFO> Epoch 370, loss 0.6666
2021-05-25 22:27:32,674: <INFO> Epoch 371, loss 0.6109
2021-05-25 22:27:32,675: <INFO> Epoch 372, loss 0.6381
2021-05-25 22:27:32,676: <INFO> Epoch 373, loss 0.6198
2021-05-25 22:27:32,677: <INFO> Epoch 374, loss 0.6152
2021-05-25 22:27:32,678: <INFO> Epoch 375, loss 0.6292
2021-05-25 22:27:32,679: <INFO> Epoch 376, loss 0.6389
2021-05-25 22:27:32,679: <INFO> Epoch 377, loss 0.6139
2021-05-25 22:27:32,680: <INFO> Epoch 378, loss 0.6306
2021-05-25 22:27:32,680: <INFO> Epoch 379, loss 0.6152
2021-05-25 22:27:32,680: <INFO> Epoch 380, loss 0.6482
2021-05-25 22:27:32,681: <INFO> Epoch 381, loss 0.6396
2021-05-25 22:27:32,681: <INFO> Epoch 382, loss 0.6400
2021-05-25 22:27:32,681: <INFO> Epoch 383, loss 0.6301
2021-05-25 22:27:32,681: <INFO> Epoch 384, loss 0.6353
2021-05-25 22:27:32,682: <INFO> Epoch 385, loss 0.6282
2021-05-25 22:27:32,682: <INFO> Epoch 386, loss 0.5972
2021-05-25 22:27:32,683: <INFO> Epoch 387, loss 0.6024
2021-05-25 22:27:32,683: <INFO> Epoch 388, loss 0.6397
2021-05-25 22:27:32,683: <INFO> Epoch 389, loss 0.6365
2021-05-25 22:27:32,683: <INFO> Epoch 390, loss 0.6130
2021-05-25 22:27:32,684: <INFO> Epoch 391, loss 0.6367
2021-05-25 22:27:32,684: <INFO> Epoch 392, loss 0.6107
2021-05-25 22:27:32,684: <INFO> Epoch 393, loss 0.6433
2021-05-25 22:27:32,684: <INFO> Epoch 394, loss 0.6670
2021-05-25 22:27:32,685: <INFO> Epoch 395, loss 0.6120
2021-05-25 22:27:32,685: <INFO> Epoch 396, loss 0.6292
2021-05-25 22:27:32,686: <INFO> Epoch 397, loss 0.6302
2021-05-25 22:27:32,688: <INFO> Epoch 398, loss 0.6331
2021-05-25 22:27:32,689: <INFO> Epoch 399, loss 0.6250
2021-05-25 22:27:32,690: <INFO> Training model: attempt 1
2021-05-25 22:27:32,748: <INFO> Model: "efficient"
2021-05-25 22:27:32,749: <INFO> _________________________________________________________________
2021-05-25 22:27:32,749: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-25 22:27:32,750: <INFO> =================================================================
2021-05-25 22:27:32,751: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-25 22:27:32,753: <INFO> _________________________________________________________________
2021-05-25 22:27:32,755: <INFO> dropout (Dropout)            (None, 1280)              0         
2021-05-25 22:27:32,755: <INFO> _________________________________________________________________
2021-05-25 22:27:32,757: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-25 22:27:32,757: <INFO> _________________________________________________________________
2021-05-25 22:27:32,758: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-25 22:27:32,759: <INFO> =================================================================
2021-05-25 22:27:32,761: <INFO> Total params: 12,810
2021-05-25 22:27:32,761: <INFO> Trainable params: 12,810
2021-05-25 22:27:32,762: <INFO> Non-trainable params: 0
2021-05-25 22:27:32,762: <INFO> _________________________________________________________________
2021-05-25 22:28:41,935: <INFO> Epoch 0, loss 2.1564
2021-05-25 22:28:41,936: <INFO> Epoch 1, loss 1.8264
2021-05-25 22:28:41,936: <INFO> Epoch 2, loss 1.6716
2021-05-25 22:28:41,937: <INFO> Epoch 3, loss 1.6239
2021-05-25 22:28:41,937: <INFO> Epoch 4, loss 1.5380
2021-05-25 22:28:41,938: <INFO> Epoch 5, loss 1.4990
2021-05-25 22:28:41,938: <INFO> Epoch 6, loss 1.4589
2021-05-25 22:28:41,938: <INFO> Epoch 7, loss 1.4343
2021-05-25 22:28:41,939: <INFO> Epoch 8, loss 1.4038
2021-05-25 22:28:41,939: <INFO> Epoch 9, loss 1.3646
2021-05-25 22:28:41,940: <INFO> Epoch 10, loss 1.3605
2021-05-25 22:28:41,940: <INFO> Epoch 11, loss 1.3190
2021-05-25 22:28:41,940: <INFO> Epoch 12, loss 1.3170
2021-05-25 22:28:41,941: <INFO> Epoch 13, loss 1.2711
2021-05-25 22:28:41,941: <INFO> Epoch 14, loss 1.2525
2021-05-25 22:28:41,941: <INFO> Epoch 15, loss 1.2410
2021-05-25 22:28:41,942: <INFO> Epoch 16, loss 1.2182
2021-05-25 22:28:41,943: <INFO> Epoch 17, loss 1.2200
2021-05-25 22:28:41,943: <INFO> Epoch 18, loss 1.2025
2021-05-25 22:28:41,943: <INFO> Epoch 19, loss 1.1700
2021-05-25 22:28:41,944: <INFO> Epoch 20, loss 1.1624
2021-05-25 22:28:41,944: <INFO> Epoch 21, loss 1.1674
2021-05-25 22:28:41,944: <INFO> Epoch 22, loss 1.1756
2021-05-25 22:28:41,944: <INFO> Epoch 23, loss 1.1484
2021-05-25 22:28:41,945: <INFO> Epoch 24, loss 1.1284
2021-05-25 22:28:41,945: <INFO> Epoch 25, loss 1.1217
2021-05-25 22:28:41,945: <INFO> Epoch 26, loss 1.1344
2021-05-25 22:28:41,947: <INFO> Epoch 27, loss 1.1049
2021-05-25 22:28:41,948: <INFO> Epoch 28, loss 1.0992
2021-05-25 22:28:41,949: <INFO> Epoch 29, loss 1.0848
2021-05-25 22:28:41,950: <INFO> Epoch 30, loss 1.1149
2021-05-25 22:28:41,950: <INFO> Epoch 31, loss 1.0915
2021-05-25 22:28:41,951: <INFO> Epoch 32, loss 1.0830
2021-05-25 22:28:41,952: <INFO> Epoch 33, loss 1.0639
2021-05-25 22:28:41,954: <INFO> Epoch 34, loss 1.0403
2021-05-25 22:28:41,955: <INFO> Epoch 35, loss 1.0299
2021-05-25 22:28:41,955: <INFO> Epoch 36, loss 1.0529
2021-05-25 22:28:41,956: <INFO> Epoch 37, loss 1.0584
2021-05-25 22:28:41,957: <INFO> Epoch 38, loss 1.0064
2021-05-25 22:28:41,958: <INFO> Epoch 39, loss 1.0420
2021-05-25 22:28:41,958: <INFO> Epoch 40, loss 1.0162
2021-05-25 22:28:41,959: <INFO> Epoch 41, loss 0.9899
2021-05-25 22:28:41,960: <INFO> Epoch 42, loss 0.9983
2021-05-25 22:28:41,961: <INFO> Epoch 43, loss 0.9911
2021-05-25 22:28:41,961: <INFO> Epoch 44, loss 1.0053
2021-05-25 22:28:41,962: <INFO> Epoch 45, loss 0.9762
2021-05-25 22:28:41,963: <INFO> Epoch 46, loss 0.9770
2021-05-25 22:28:41,964: <INFO> Epoch 47, loss 0.9872
2021-05-25 22:28:41,964: <INFO> Epoch 48, loss 0.9824
2021-05-25 22:28:41,965: <INFO> Epoch 49, loss 0.9716
2021-05-25 22:28:41,965: <INFO> Epoch 50, loss 0.9844
2021-05-25 22:28:41,966: <INFO> Epoch 51, loss 0.9822
2021-05-25 22:28:41,967: <INFO> Epoch 52, loss 0.9640
2021-05-25 22:28:41,968: <INFO> Epoch 53, loss 0.9546
2021-05-25 22:28:41,968: <INFO> Epoch 54, loss 0.9407
2021-05-25 22:28:41,970: <INFO> Epoch 55, loss 0.9399
2021-05-25 22:28:41,972: <INFO> Epoch 56, loss 0.9388
2021-05-25 22:28:41,973: <INFO> Epoch 57, loss 0.9090
2021-05-25 22:28:41,974: <INFO> Epoch 58, loss 0.9038
2021-05-25 22:28:41,975: <INFO> Epoch 59, loss 0.9213
2021-05-25 22:28:41,976: <INFO> Epoch 60, loss 0.9073
2021-05-25 22:28:41,977: <INFO> Epoch 61, loss 0.9165
2021-05-25 22:28:41,977: <INFO> Epoch 62, loss 0.9275
2021-05-25 22:28:41,978: <INFO> Epoch 63, loss 0.8899
2021-05-25 22:28:41,979: <INFO> Epoch 64, loss 0.8932
2021-05-25 22:28:41,979: <INFO> Epoch 65, loss 0.9234
2021-05-25 22:28:41,980: <INFO> Epoch 66, loss 0.9107
2021-05-25 22:28:41,981: <INFO> Epoch 67, loss 0.9072
2021-05-25 22:28:41,982: <INFO> Epoch 68, loss 0.8825
2021-05-25 22:28:41,982: <INFO> Epoch 69, loss 0.8820
2021-05-25 22:28:41,983: <INFO> Epoch 70, loss 0.8790
2021-05-25 22:28:41,984: <INFO> Epoch 71, loss 0.9175
2021-05-25 22:28:41,984: <INFO> Epoch 72, loss 0.8888
2021-05-25 22:28:41,984: <INFO> Epoch 73, loss 0.8909
2021-05-25 22:28:41,985: <INFO> Epoch 74, loss 0.8880
2021-05-25 22:28:41,989: <INFO> Epoch 75, loss 0.8656
2021-05-25 22:28:41,989: <INFO> Epoch 76, loss 0.8836
2021-05-25 22:28:41,990: <INFO> Epoch 77, loss 0.8588
2021-05-25 22:28:41,991: <INFO> Epoch 78, loss 0.8631
2021-05-25 22:28:41,991: <INFO> Epoch 79, loss 0.8846
2021-05-25 22:28:41,992: <INFO> Epoch 80, loss 0.8579
2021-05-25 22:28:41,993: <INFO> Epoch 81, loss 0.8600
2021-05-25 22:28:41,994: <INFO> Epoch 82, loss 0.8610
2021-05-25 22:28:41,994: <INFO> Epoch 83, loss 0.8611
2021-05-25 22:28:41,995: <INFO> Epoch 84, loss 0.8481
2021-05-25 22:28:41,995: <INFO> Epoch 85, loss 0.8527
2021-05-25 22:28:41,996: <INFO> Epoch 86, loss 0.8637
2021-05-25 22:28:41,997: <INFO> Epoch 87, loss 0.8454
2021-05-25 22:28:41,998: <INFO> Epoch 88, loss 0.8420
2021-05-25 22:28:41,998: <INFO> Epoch 89, loss 0.8106
2021-05-25 22:28:41,999: <INFO> Epoch 90, loss 0.8500
2021-05-25 22:28:42,000: <INFO> Epoch 91, loss 0.8465
2021-05-25 22:28:42,000: <INFO> Epoch 92, loss 0.8297
2021-05-25 22:28:42,001: <INFO> Epoch 93, loss 0.8407
2021-05-25 22:28:42,003: <INFO> Epoch 94, loss 0.8294
2021-05-25 22:28:42,005: <INFO> Epoch 95, loss 0.8196
2021-05-25 22:28:42,006: <INFO> Epoch 96, loss 0.8116
2021-05-25 22:28:42,007: <INFO> Epoch 97, loss 0.8210
2021-05-25 22:28:42,007: <INFO> Epoch 98, loss 0.8334
2021-05-25 22:28:42,008: <INFO> Epoch 99, loss 0.8133
2021-05-25 22:28:42,009: <INFO> Epoch 100, loss 0.8114
2021-05-25 22:28:42,010: <INFO> Epoch 101, loss 0.8212
2021-05-25 22:28:42,010: <INFO> Epoch 102, loss 0.7946
2021-05-25 22:28:42,011: <INFO> Epoch 103, loss 0.8181
2021-05-25 22:28:42,012: <INFO> Epoch 104, loss 0.8189
2021-05-25 22:28:42,012: <INFO> Epoch 105, loss 0.8262
2021-05-25 22:28:42,013: <INFO> Epoch 106, loss 0.8200
2021-05-25 22:28:42,014: <INFO> Epoch 107, loss 0.8103
2021-05-25 22:28:42,014: <INFO> Epoch 108, loss 0.8209
2021-05-25 22:28:42,015: <INFO> Epoch 109, loss 0.8136
2021-05-25 22:28:42,015: <INFO> Epoch 110, loss 0.8259
2021-05-25 22:28:42,016: <INFO> Epoch 111, loss 0.7941
2021-05-25 22:28:42,017: <INFO> Epoch 112, loss 0.8055
2021-05-25 22:28:42,017: <INFO> Epoch 113, loss 0.8054
2021-05-25 22:28:42,018: <INFO> Epoch 114, loss 0.7765
2021-05-25 22:28:42,020: <INFO> Epoch 115, loss 0.7911
2021-05-25 22:28:42,021: <INFO> Epoch 116, loss 0.7683
2021-05-25 22:28:42,021: <INFO> Epoch 117, loss 0.8074
2021-05-25 22:28:42,022: <INFO> Epoch 118, loss 0.7804
2021-05-25 22:28:42,023: <INFO> Epoch 119, loss 0.7836
2021-05-25 22:28:42,023: <INFO> Epoch 120, loss 0.7918
2021-05-25 22:28:42,024: <INFO> Epoch 121, loss 0.7961
2021-05-25 22:28:42,024: <INFO> Epoch 122, loss 0.7557
2021-05-25 22:28:42,025: <INFO> Epoch 123, loss 0.8099
2021-05-25 22:28:42,026: <INFO> Epoch 124, loss 0.7792
2021-05-25 22:28:42,027: <INFO> Epoch 125, loss 0.7791
2021-05-25 22:28:42,028: <INFO> Epoch 126, loss 0.7712
2021-05-25 22:28:42,028: <INFO> Epoch 127, loss 0.7683
2021-05-25 22:28:42,029: <INFO> Epoch 128, loss 0.7612
2021-05-25 22:28:42,030: <INFO> Epoch 129, loss 0.7672
2021-05-25 22:28:42,030: <INFO> Epoch 130, loss 0.7718
2021-05-25 22:28:42,031: <INFO> Epoch 131, loss 0.7601
2021-05-25 22:28:42,032: <INFO> Epoch 132, loss 0.7434
2021-05-25 22:28:42,032: <INFO> Epoch 133, loss 0.7462
2021-05-25 22:28:42,033: <INFO> Epoch 134, loss 0.7720
2021-05-25 22:28:42,033: <INFO> Epoch 135, loss 0.7656
2021-05-25 22:28:42,034: <INFO> Epoch 136, loss 0.7600
2021-05-25 22:28:42,034: <INFO> Epoch 137, loss 0.7634
2021-05-25 22:28:42,035: <INFO> Epoch 138, loss 0.7556
2021-05-25 22:28:42,035: <INFO> Epoch 139, loss 0.7345
2021-05-25 22:28:42,037: <INFO> Epoch 140, loss 0.7511
2021-05-25 22:28:42,038: <INFO> Epoch 141, loss 0.7539
2021-05-25 22:28:42,039: <INFO> Epoch 142, loss 0.7303
2021-05-25 22:28:42,040: <INFO> Epoch 143, loss 0.7562
2021-05-25 22:28:42,040: <INFO> Epoch 144, loss 0.7267
2021-05-25 22:28:42,041: <INFO> Epoch 145, loss 0.7680
2021-05-25 22:28:42,041: <INFO> Epoch 146, loss 0.7367
2021-05-25 22:28:42,042: <INFO> Epoch 147, loss 0.7775
2021-05-25 22:28:42,042: <INFO> Epoch 148, loss 0.7559
2021-05-25 22:28:42,043: <INFO> Epoch 149, loss 0.7420
2021-05-25 22:28:42,044: <INFO> Epoch 150, loss 0.7548
2021-05-25 22:28:42,044: <INFO> Epoch 151, loss 0.7167
2021-05-25 22:28:42,044: <INFO> Epoch 152, loss 0.7609
2021-05-25 22:28:42,045: <INFO> Epoch 153, loss 0.7348
2021-05-25 22:28:42,045: <INFO> Epoch 154, loss 0.7353
2021-05-25 22:28:42,046: <INFO> Epoch 155, loss 0.7835
2021-05-25 22:28:42,047: <INFO> Epoch 156, loss 0.7401
2021-05-25 22:28:42,047: <INFO> Epoch 157, loss 0.7517
2021-05-25 22:28:42,048: <INFO> Epoch 158, loss 0.7333
2021-05-25 22:28:42,048: <INFO> Epoch 159, loss 0.7435
2021-05-25 22:28:42,049: <INFO> Epoch 160, loss 0.7420
2021-05-25 22:28:42,050: <INFO> Epoch 161, loss 0.7284
2021-05-25 22:28:42,050: <INFO> Epoch 162, loss 0.7174
2021-05-25 22:28:42,051: <INFO> Epoch 163, loss 0.7444
2021-05-25 22:28:42,051: <INFO> Epoch 164, loss 0.7377
2021-05-25 22:28:42,052: <INFO> Epoch 165, loss 0.7269
2021-05-25 22:28:42,054: <INFO> Epoch 166, loss 0.7087
2021-05-25 22:28:42,055: <INFO> Epoch 167, loss 0.7072
2021-05-25 22:28:42,056: <INFO> Epoch 168, loss 0.7438
2021-05-25 22:28:42,056: <INFO> Epoch 169, loss 0.7295
2021-05-25 22:28:42,058: <INFO> Epoch 170, loss 0.7309
2021-05-25 22:28:42,058: <INFO> Epoch 171, loss 0.7366
2021-05-25 22:28:42,059: <INFO> Epoch 172, loss 0.7009
2021-05-25 22:28:42,060: <INFO> Epoch 173, loss 0.7186
2021-05-25 22:28:42,060: <INFO> Epoch 174, loss 0.7361
2021-05-25 22:28:42,061: <INFO> Epoch 175, loss 0.7388
2021-05-25 22:28:42,061: <INFO> Epoch 176, loss 0.7053
2021-05-25 22:28:42,062: <INFO> Epoch 177, loss 0.7137
2021-05-25 22:28:42,062: <INFO> Epoch 178, loss 0.7208
2021-05-25 22:28:42,063: <INFO> Epoch 179, loss 0.7052
2021-05-25 22:28:42,063: <INFO> Epoch 180, loss 0.7412
2021-05-25 22:28:42,064: <INFO> Epoch 181, loss 0.6801
2021-05-25 22:28:42,065: <INFO> Epoch 182, loss 0.7125
2021-05-25 22:28:42,065: <INFO> Epoch 183, loss 0.7118
2021-05-25 22:28:42,066: <INFO> Epoch 184, loss 0.7105
2021-05-25 22:28:42,066: <INFO> Epoch 185, loss 0.7080
2021-05-25 22:28:42,066: <INFO> Epoch 186, loss 0.6970
2021-05-25 22:28:42,066: <INFO> Epoch 187, loss 0.7232
2021-05-25 22:28:42,067: <INFO> Epoch 188, loss 0.7331
2021-05-25 22:28:42,068: <INFO> Epoch 189, loss 0.7091
2021-05-25 22:28:42,069: <INFO> Epoch 190, loss 0.6936
2021-05-25 22:28:42,071: <INFO> Epoch 191, loss 0.6916
2021-05-25 22:28:42,072: <INFO> Epoch 192, loss 0.7320
2021-05-25 22:28:42,073: <INFO> Epoch 193, loss 0.7002
2021-05-25 22:28:42,073: <INFO> Epoch 194, loss 0.7137
2021-05-25 22:28:42,074: <INFO> Epoch 195, loss 0.6882
2021-05-25 22:28:42,075: <INFO> Epoch 196, loss 0.7022
2021-05-25 22:28:42,075: <INFO> Epoch 197, loss 0.7140
2021-05-25 22:28:42,075: <INFO> Epoch 198, loss 0.7118
2021-05-25 22:28:42,076: <INFO> Epoch 199, loss 0.7163
2021-05-25 22:28:42,076: <INFO> Epoch 200, loss 0.6995
2021-05-25 22:28:42,077: <INFO> Epoch 201, loss 0.6796
2021-05-25 22:28:42,077: <INFO> Epoch 202, loss 0.7086
2021-05-25 22:28:42,078: <INFO> Epoch 203, loss 0.7180
2021-05-25 22:28:42,078: <INFO> Epoch 204, loss 0.7001
2021-05-25 22:28:42,079: <INFO> Epoch 205, loss 0.6844
2021-05-25 22:28:42,080: <INFO> Epoch 206, loss 0.7043
2021-05-25 22:28:42,080: <INFO> Epoch 207, loss 0.7038
2021-05-25 22:28:42,081: <INFO> Epoch 208, loss 0.7085
2021-05-25 22:28:42,081: <INFO> Epoch 209, loss 0.7171
2021-05-25 22:28:42,082: <INFO> Epoch 210, loss 0.6789
2021-05-25 22:28:42,083: <INFO> Epoch 211, loss 0.6852
2021-05-25 22:28:42,083: <INFO> Epoch 212, loss 0.6821
2021-05-25 22:28:42,084: <INFO> Epoch 213, loss 0.6674
2021-05-25 22:28:42,084: <INFO> Epoch 214, loss 0.6691
2021-05-25 22:28:42,085: <INFO> Epoch 215, loss 0.6679
2021-05-25 22:28:42,086: <INFO> Epoch 216, loss 0.6949
2021-05-25 22:28:42,087: <INFO> Epoch 217, loss 0.6928
2021-05-25 22:28:42,087: <INFO> Epoch 218, loss 0.6973
2021-05-25 22:28:42,088: <INFO> Epoch 219, loss 0.6899
2021-05-25 22:28:42,089: <INFO> Epoch 220, loss 0.6944
2021-05-25 22:28:42,089: <INFO> Epoch 221, loss 0.7009
2021-05-25 22:28:42,090: <INFO> Epoch 222, loss 0.7015
2021-05-25 22:28:42,091: <INFO> Epoch 223, loss 0.6755
2021-05-25 22:28:42,091: <INFO> Epoch 224, loss 0.6889
2021-05-25 22:28:42,092: <INFO> Epoch 225, loss 0.7204
2021-05-25 22:28:42,092: <INFO> Epoch 226, loss 0.7087
2021-05-25 22:28:42,093: <INFO> Epoch 227, loss 0.7099
2021-05-25 22:28:42,094: <INFO> Epoch 228, loss 0.6442
2021-05-25 22:28:42,094: <INFO> Epoch 229, loss 0.6968
2021-05-25 22:28:42,095: <INFO> Epoch 230, loss 0.6792
2021-05-25 22:28:42,095: <INFO> Epoch 231, loss 0.6970
2021-05-25 22:28:42,096: <INFO> Epoch 232, loss 0.6912
2021-05-25 22:28:42,096: <INFO> Epoch 233, loss 0.6865
2021-05-25 22:28:42,097: <INFO> Epoch 234, loss 0.6694
2021-05-25 22:28:42,097: <INFO> Epoch 235, loss 0.6708
2021-05-25 22:28:42,098: <INFO> Epoch 236, loss 0.6731
2021-05-25 22:28:42,098: <INFO> Epoch 237, loss 0.6950
2021-05-25 22:28:42,099: <INFO> Epoch 238, loss 0.7038
2021-05-25 22:28:42,100: <INFO> Epoch 239, loss 0.6955
2021-05-25 22:28:42,100: <INFO> Epoch 240, loss 0.6890
2021-05-25 22:28:42,101: <INFO> Epoch 241, loss 0.6770
2021-05-25 22:28:42,102: <INFO> Epoch 242, loss 0.6664
2021-05-25 22:28:42,103: <INFO> Epoch 243, loss 0.6671
2021-05-25 22:28:42,104: <INFO> Epoch 244, loss 0.6844
2021-05-25 22:28:42,104: <INFO> Epoch 245, loss 0.7031
2021-05-25 22:28:42,105: <INFO> Epoch 246, loss 0.6980
2021-05-25 22:28:42,105: <INFO> Epoch 247, loss 0.6856
2021-05-25 22:28:42,106: <INFO> Epoch 248, loss 0.6851
2021-05-25 22:28:42,106: <INFO> Epoch 249, loss 0.6979
2021-05-25 22:28:42,106: <INFO> Epoch 250, loss 0.6793
2021-05-25 22:28:42,107: <INFO> Epoch 251, loss 0.6913
2021-05-25 22:28:42,108: <INFO> Epoch 252, loss 0.6606
2021-05-25 22:28:42,108: <INFO> Epoch 253, loss 0.6771
2021-05-25 22:28:42,109: <INFO> Epoch 254, loss 0.6727
2021-05-25 22:28:42,109: <INFO> Epoch 255, loss 0.6721
2021-05-25 22:28:42,110: <INFO> Epoch 256, loss 0.6852
2021-05-25 22:28:42,111: <INFO> Epoch 257, loss 0.6481
2021-05-25 22:28:42,111: <INFO> Epoch 258, loss 0.6609
2021-05-25 22:28:42,112: <INFO> Epoch 259, loss 0.6443
2021-05-25 22:28:42,112: <INFO> Epoch 260, loss 0.6726
2021-05-25 22:28:42,113: <INFO> Epoch 261, loss 0.6987
2021-05-25 22:28:42,113: <INFO> Epoch 262, loss 0.6588
2021-05-25 22:28:42,114: <INFO> Epoch 263, loss 0.6708
2021-05-25 22:28:42,115: <INFO> Epoch 264, loss 0.6471
2021-05-25 22:28:42,115: <INFO> Epoch 265, loss 0.6589
2021-05-25 22:28:42,115: <INFO> Epoch 266, loss 0.6562
2021-05-25 22:28:42,116: <INFO> Epoch 267, loss 0.6721
2021-05-25 22:28:42,116: <INFO> Epoch 268, loss 0.6815
2021-05-25 22:28:42,117: <INFO> Epoch 269, loss 0.6574
2021-05-25 22:28:42,117: <INFO> Epoch 270, loss 0.6475
2021-05-25 22:28:42,118: <INFO> Epoch 271, loss 0.6614
2021-05-25 22:28:42,118: <INFO> Epoch 272, loss 0.7093
2021-05-25 22:28:42,121: <INFO> Epoch 273, loss 0.6698
2021-05-25 22:28:42,122: <INFO> Epoch 274, loss 0.6574
2021-05-25 22:28:42,122: <INFO> Epoch 275, loss 0.6635
2021-05-25 22:28:42,123: <INFO> Epoch 276, loss 0.6794
2021-05-25 22:28:42,124: <INFO> Epoch 277, loss 0.6800
2021-05-25 22:28:42,124: <INFO> Epoch 278, loss 0.6697
2021-05-25 22:28:42,125: <INFO> Epoch 279, loss 0.6657
2021-05-25 22:28:42,125: <INFO> Epoch 280, loss 0.6686
2021-05-25 22:28:42,126: <INFO> Epoch 281, loss 0.6664
2021-05-25 22:28:42,126: <INFO> Epoch 282, loss 0.6492
2021-05-25 22:28:42,126: <INFO> Epoch 283, loss 0.6326
2021-05-25 22:28:42,127: <INFO> Epoch 284, loss 0.6354
2021-05-25 22:28:42,128: <INFO> Epoch 285, loss 0.6748
2021-05-25 22:28:42,129: <INFO> Epoch 286, loss 0.6846
2021-05-25 22:28:42,129: <INFO> Epoch 287, loss 0.6729
2021-05-25 22:28:42,130: <INFO> Epoch 288, loss 0.6722
2021-05-25 22:28:42,131: <INFO> Epoch 289, loss 0.6643
2021-05-25 22:28:42,131: <INFO> Epoch 290, loss 0.6670
2021-05-25 22:28:42,131: <INFO> Epoch 291, loss 0.6599
2021-05-25 22:28:42,132: <INFO> Epoch 292, loss 0.6992
2021-05-25 22:28:42,132: <INFO> Epoch 293, loss 0.6558
2021-05-25 22:28:42,132: <INFO> Epoch 294, loss 0.6376
2021-05-25 22:28:42,133: <INFO> Epoch 295, loss 0.6719
2021-05-25 22:28:42,133: <INFO> Epoch 296, loss 0.6491
2021-05-25 22:28:42,134: <INFO> Epoch 297, loss 0.6614
2021-05-25 22:28:42,134: <INFO> Epoch 298, loss 0.6499
2021-05-25 22:28:42,134: <INFO> Epoch 299, loss 0.6596
2021-05-25 22:28:42,135: <INFO> Epoch 300, loss 0.6543
2021-05-25 22:28:42,135: <INFO> Epoch 301, loss 0.6391
2021-05-25 22:28:42,136: <INFO> Epoch 302, loss 0.6360
2021-05-25 22:28:42,136: <INFO> Epoch 303, loss 0.6491
2021-05-25 22:28:42,136: <INFO> Epoch 304, loss 0.6649
2021-05-25 22:28:42,137: <INFO> Epoch 305, loss 0.6462
2021-05-25 22:28:42,137: <INFO> Epoch 306, loss 0.6468
2021-05-25 22:28:42,137: <INFO> Epoch 307, loss 0.6507
2021-05-25 22:28:42,138: <INFO> Epoch 308, loss 0.6502
2021-05-25 22:28:42,138: <INFO> Epoch 309, loss 0.6870
2021-05-25 22:28:42,139: <INFO> Epoch 310, loss 0.6327
2021-05-25 22:28:42,139: <INFO> Epoch 311, loss 0.6565
2021-05-25 22:28:42,139: <INFO> Epoch 312, loss 0.6430
2021-05-25 22:28:42,140: <INFO> Epoch 313, loss 0.6471
2021-05-25 22:28:42,140: <INFO> Epoch 314, loss 0.6582
2021-05-25 22:28:42,140: <INFO> Epoch 315, loss 0.6468
2021-05-25 22:28:42,141: <INFO> Epoch 316, loss 0.6439
2021-05-25 22:28:42,141: <INFO> Epoch 317, loss 0.6218
2021-05-25 22:28:42,141: <INFO> Epoch 318, loss 0.6630
2021-05-25 22:28:42,142: <INFO> Epoch 319, loss 0.6580
2021-05-25 22:28:42,142: <INFO> Epoch 320, loss 0.6571
2021-05-25 22:28:42,143: <INFO> Epoch 321, loss 0.6169
2021-05-25 22:28:42,143: <INFO> Epoch 322, loss 0.6419
2021-05-25 22:28:42,143: <INFO> Epoch 323, loss 0.6333
2021-05-25 22:28:42,144: <INFO> Epoch 324, loss 0.6519
2021-05-25 22:28:42,144: <INFO> Epoch 325, loss 0.6385
2021-05-25 22:28:42,144: <INFO> Epoch 326, loss 0.6565
2021-05-25 22:28:42,145: <INFO> Epoch 327, loss 0.6624
2021-05-25 22:28:42,145: <INFO> Epoch 328, loss 0.6097
2021-05-25 22:28:42,146: <INFO> Epoch 329, loss 0.6413
2021-05-25 22:28:42,146: <INFO> Epoch 330, loss 0.6578
2021-05-25 22:28:42,147: <INFO> Epoch 331, loss 0.6270
2021-05-25 22:28:42,147: <INFO> Epoch 332, loss 0.6336
2021-05-25 22:28:42,147: <INFO> Epoch 333, loss 0.6212
2021-05-25 22:28:42,148: <INFO> Epoch 334, loss 0.6269
2021-05-25 22:28:42,148: <INFO> Epoch 335, loss 0.6164
2021-05-25 22:28:42,148: <INFO> Epoch 336, loss 0.6310
2021-05-25 22:28:42,149: <INFO> Epoch 337, loss 0.6573
2021-05-25 22:28:42,150: <INFO> Epoch 338, loss 0.6337
2021-05-25 22:28:42,151: <INFO> Epoch 339, loss 0.6424
2021-05-25 22:28:42,153: <INFO> Epoch 340, loss 0.6238
2021-05-25 22:28:42,154: <INFO> Epoch 341, loss 0.6520
2021-05-25 22:28:42,155: <INFO> Epoch 342, loss 0.6402
2021-05-25 22:28:42,156: <INFO> Epoch 343, loss 0.6128
2021-05-25 22:28:42,157: <INFO> Epoch 344, loss 0.6495
2021-05-25 22:28:42,158: <INFO> Epoch 345, loss 0.6250
2021-05-25 22:28:42,159: <INFO> Epoch 346, loss 0.6535
2021-05-25 22:28:42,159: <INFO> Epoch 347, loss 0.6274
2021-05-25 22:28:42,159: <INFO> Epoch 348, loss 0.6673
2021-05-25 22:28:42,160: <INFO> Epoch 349, loss 0.6354
2021-05-25 22:28:42,161: <INFO> Epoch 350, loss 0.6405
2021-05-25 22:28:42,162: <INFO> Epoch 351, loss 0.6308
2021-05-25 22:28:42,162: <INFO> Epoch 352, loss 0.6333
2021-05-25 22:28:42,163: <INFO> Epoch 353, loss 0.6420
2021-05-25 22:28:42,164: <INFO> Epoch 354, loss 0.6429
2021-05-25 22:28:42,164: <INFO> Epoch 355, loss 0.6516
2021-05-25 22:28:42,165: <INFO> Epoch 356, loss 0.6559
2021-05-25 22:28:42,165: <INFO> Epoch 357, loss 0.6500
2021-05-25 22:28:42,166: <INFO> Epoch 358, loss 0.6599
2021-05-25 22:28:42,166: <INFO> Epoch 359, loss 0.6330
2021-05-25 22:28:42,167: <INFO> Epoch 360, loss 0.6563
2021-05-25 22:28:42,168: <INFO> Epoch 361, loss 0.6483
2021-05-25 22:28:42,168: <INFO> Epoch 362, loss 0.6643
2021-05-25 22:28:42,169: <INFO> Epoch 363, loss 0.6260
2021-05-25 22:28:42,169: <INFO> Epoch 364, loss 0.6355
2021-05-25 22:28:42,170: <INFO> Epoch 365, loss 0.6518
2021-05-25 22:28:42,172: <INFO> Epoch 366, loss 0.6230
2021-05-25 22:28:42,173: <INFO> Epoch 367, loss 0.6457
2021-05-25 22:28:42,174: <INFO> Epoch 368, loss 0.6470
2021-05-25 22:28:42,174: <INFO> Epoch 369, loss 0.6350
2021-05-25 22:28:42,175: <INFO> Epoch 370, loss 0.6398
2021-05-25 22:28:42,175: <INFO> Epoch 371, loss 0.6375
2021-05-25 22:28:42,176: <INFO> Epoch 372, loss 0.6074
2021-05-25 22:28:42,177: <INFO> Epoch 373, loss 0.6205
2021-05-25 22:28:42,177: <INFO> Epoch 374, loss 0.6202
2021-05-25 22:28:42,178: <INFO> Epoch 375, loss 0.6334
2021-05-25 22:28:42,178: <INFO> Epoch 376, loss 0.6156
2021-05-25 22:28:42,179: <INFO> Epoch 377, loss 0.6349
2021-05-25 22:28:42,179: <INFO> Epoch 378, loss 0.6436
2021-05-25 22:28:42,179: <INFO> Epoch 379, loss 0.6284
2021-05-25 22:28:42,180: <INFO> Epoch 380, loss 0.6326
2021-05-25 22:28:42,181: <INFO> Epoch 381, loss 0.5824
2021-05-25 22:28:42,181: <INFO> Epoch 382, loss 0.6381
2021-05-25 22:28:42,182: <INFO> Epoch 383, loss 0.6337
2021-05-25 22:28:42,183: <INFO> Epoch 384, loss 0.6497
2021-05-25 22:28:42,183: <INFO> Epoch 385, loss 0.6511
2021-05-25 22:28:42,184: <INFO> Epoch 386, loss 0.6139
2021-05-25 22:28:42,184: <INFO> Epoch 387, loss 0.6253
2021-05-25 22:28:42,185: <INFO> Epoch 388, loss 0.6151
2021-05-25 22:28:42,186: <INFO> Epoch 389, loss 0.6426
2021-05-25 22:28:42,186: <INFO> Epoch 390, loss 0.6363
2021-05-25 22:28:42,187: <INFO> Epoch 391, loss 0.6441
2021-05-25 22:28:42,188: <INFO> Epoch 392, loss 0.6325
2021-05-25 22:28:42,188: <INFO> Epoch 393, loss 0.6595
2021-05-25 22:28:42,189: <INFO> Epoch 394, loss 0.6260
2021-05-25 22:28:42,189: <INFO> Epoch 395, loss 0.6365
2021-05-25 22:28:42,189: <INFO> Epoch 396, loss 0.6062
2021-05-25 22:28:42,190: <INFO> Epoch 397, loss 0.6058
2021-05-25 22:28:42,191: <INFO> Epoch 398, loss 0.6583
2021-05-25 22:28:42,191: <INFO> Epoch 399, loss 0.6483
2021-05-25 22:28:42,192: <INFO> Training model: attempt 2
2021-05-25 22:28:42,249: <INFO> Model: "efficient"
2021-05-25 22:28:42,250: <INFO> _________________________________________________________________
2021-05-25 22:28:42,250: <INFO> Layer (type)                 Output Shape              Param #   
2021-05-25 22:28:42,251: <INFO> =================================================================
2021-05-25 22:28:42,251: <INFO> pool (GlobalAveragePooling2D (None, 1280)              0         
2021-05-25 22:28:42,252: <INFO> _________________________________________________________________
2021-05-25 22:28:42,253: <INFO> dropout (Dropout)            (None, 1280)              0         
2021-05-25 22:28:42,254: <INFO> _________________________________________________________________
2021-05-25 22:28:42,255: <INFO> dense (Dense)                (None, 10)                12810     
2021-05-25 22:28:42,255: <INFO> _________________________________________________________________
2021-05-25 22:28:42,256: <INFO> softmax (Softmax)            (None, 10)                0         
2021-05-25 22:28:42,256: <INFO> =================================================================
2021-05-25 22:28:42,258: <INFO> Total params: 12,810
2021-05-25 22:28:42,258: <INFO> Trainable params: 12,810
2021-05-25 22:28:42,258: <INFO> Non-trainable params: 0
2021-05-25 22:28:42,259: <INFO> _________________________________________________________________
2021-05-25 22:29:51,451: <INFO> Epoch 0, loss 2.2842
2021-05-25 22:29:51,451: <INFO> Epoch 1, loss 1.8803
2021-05-25 22:29:51,452: <INFO> Epoch 2, loss 1.7409
2021-05-25 22:29:51,453: <INFO> Epoch 3, loss 1.6528
2021-05-25 22:29:51,453: <INFO> Epoch 4, loss 1.5938
2021-05-25 22:29:51,453: <INFO> Epoch 5, loss 1.5083
2021-05-25 22:29:51,454: <INFO> Epoch 6, loss 1.4485
2021-05-25 22:29:51,454: <INFO> Epoch 7, loss 1.4515
2021-05-25 22:29:51,455: <INFO> Epoch 8, loss 1.4217
2021-05-25 22:29:51,456: <INFO> Epoch 9, loss 1.3894
2021-05-25 22:29:51,456: <INFO> Epoch 10, loss 1.3580
2021-05-25 22:29:51,457: <INFO> Epoch 11, loss 1.3557
2021-05-25 22:29:51,458: <INFO> Epoch 12, loss 1.3036
2021-05-25 22:29:51,459: <INFO> Epoch 13, loss 1.2893
2021-05-25 22:29:51,459: <INFO> Epoch 14, loss 1.2706
2021-05-25 22:29:51,460: <INFO> Epoch 15, loss 1.2633
2021-05-25 22:29:51,460: <INFO> Epoch 16, loss 1.2523
2021-05-25 22:29:51,460: <INFO> Epoch 17, loss 1.2286
2021-05-25 22:29:51,461: <INFO> Epoch 18, loss 1.2264
2021-05-25 22:29:51,462: <INFO> Epoch 19, loss 1.2199
2021-05-25 22:29:51,462: <INFO> Epoch 20, loss 1.1861
2021-05-25 22:29:51,463: <INFO> Epoch 21, loss 1.1842
2021-05-25 22:29:51,464: <INFO> Epoch 22, loss 1.1607
2021-05-25 22:29:51,464: <INFO> Epoch 23, loss 1.1495
2021-05-25 22:29:51,465: <INFO> Epoch 24, loss 1.1386
2021-05-25 22:29:51,466: <INFO> Epoch 25, loss 1.1259
2021-05-25 22:29:51,466: <INFO> Epoch 26, loss 1.1387
2021-05-25 22:29:51,467: <INFO> Epoch 27, loss 1.1072
2021-05-25 22:29:51,467: <INFO> Epoch 28, loss 1.1261
2021-05-25 22:29:51,468: <INFO> Epoch 29, loss 1.0887
2021-05-25 22:29:51,469: <INFO> Epoch 30, loss 1.0754
2021-05-25 22:29:51,470: <INFO> Epoch 31, loss 1.0836
2021-05-25 22:29:51,470: <INFO> Epoch 32, loss 1.0764
2021-05-25 22:29:51,470: <INFO> Epoch 33, loss 1.0735
2021-05-25 22:29:51,471: <INFO> Epoch 34, loss 1.0602
2021-05-25 22:29:51,472: <INFO> Epoch 35, loss 1.0478
2021-05-25 22:29:51,472: <INFO> Epoch 36, loss 1.0349
2021-05-25 22:29:51,473: <INFO> Epoch 37, loss 1.0268
2021-05-25 22:29:51,474: <INFO> Epoch 38, loss 1.0376
2021-05-25 22:29:51,474: <INFO> Epoch 39, loss 1.0132
2021-05-25 22:29:51,475: <INFO> Epoch 40, loss 1.0372
2021-05-25 22:29:51,476: <INFO> Epoch 41, loss 1.0198
2021-05-25 22:29:51,476: <INFO> Epoch 42, loss 1.0075
2021-05-25 22:29:51,477: <INFO> Epoch 43, loss 0.9982
2021-05-25 22:29:51,477: <INFO> Epoch 44, loss 0.9929
2021-05-25 22:29:51,478: <INFO> Epoch 45, loss 0.9866
2021-05-25 22:29:51,479: <INFO> Epoch 46, loss 0.9867
2021-05-25 22:29:51,479: <INFO> Epoch 47, loss 0.9784
2021-05-25 22:29:51,479: <INFO> Epoch 48, loss 0.9607
2021-05-25 22:29:51,480: <INFO> Epoch 49, loss 0.9702
2021-05-25 22:29:51,480: <INFO> Epoch 50, loss 0.9725
2021-05-25 22:29:51,480: <INFO> Epoch 51, loss 0.9698
2021-05-25 22:29:51,480: <INFO> Epoch 52, loss 0.9767
2021-05-25 22:29:51,481: <INFO> Epoch 53, loss 0.9278
2021-05-25 22:29:51,482: <INFO> Epoch 54, loss 0.9531
2021-05-25 22:29:51,482: <INFO> Epoch 55, loss 0.9558
2021-05-25 22:29:51,483: <INFO> Epoch 56, loss 0.9302
2021-05-25 22:29:51,484: <INFO> Epoch 57, loss 0.9418
2021-05-25 22:29:51,484: <INFO> Epoch 58, loss 0.9418
2021-05-25 22:29:51,485: <INFO> Epoch 59, loss 0.9144
2021-05-25 22:29:51,487: <INFO> Epoch 60, loss 0.9295
2021-05-25 22:29:51,488: <INFO> Epoch 61, loss 0.9437
2021-05-25 22:29:51,488: <INFO> Epoch 62, loss 0.9323
2021-05-25 22:29:51,489: <INFO> Epoch 63, loss 0.9091
2021-05-25 22:29:51,490: <INFO> Epoch 64, loss 0.8975
2021-05-25 22:29:51,490: <INFO> Epoch 65, loss 0.9032
2021-05-25 22:29:51,490: <INFO> Epoch 66, loss 0.9016
2021-05-25 22:29:51,491: <INFO> Epoch 67, loss 0.8906
2021-05-25 22:29:51,491: <INFO> Epoch 68, loss 0.8909
2021-05-25 22:29:51,492: <INFO> Epoch 69, loss 0.8880
2021-05-25 22:29:51,493: <INFO> Epoch 70, loss 0.8949
2021-05-25 22:29:51,493: <INFO> Epoch 71, loss 0.8887
2021-05-25 22:29:51,494: <INFO> Epoch 72, loss 0.8776
2021-05-25 22:29:51,495: <INFO> Epoch 73, loss 0.8829
2021-05-25 22:29:51,495: <INFO> Epoch 74, loss 0.8845
2021-05-25 22:29:51,495: <INFO> Epoch 75, loss 0.8632
2021-05-25 22:29:51,496: <INFO> Epoch 76, loss 0.8748
2021-05-25 22:29:51,497: <INFO> Epoch 77, loss 0.9051
2021-05-25 22:29:51,497: <INFO> Epoch 78, loss 0.8758
2021-05-25 22:29:51,498: <INFO> Epoch 79, loss 0.8526
2021-05-25 22:29:51,499: <INFO> Epoch 80, loss 0.8636
2021-05-25 22:29:51,500: <INFO> Epoch 81, loss 0.8630
2021-05-25 22:29:51,500: <INFO> Epoch 82, loss 0.8587
2021-05-25 22:29:51,500: <INFO> Epoch 83, loss 0.8623
2021-05-25 22:29:51,500: <INFO> Epoch 84, loss 0.8508
2021-05-25 22:29:51,501: <INFO> Epoch 85, loss 0.8374
2021-05-25 22:29:51,503: <INFO> Epoch 86, loss 0.8609
2021-05-25 22:29:51,504: <INFO> Epoch 87, loss 0.8591
2021-05-25 22:29:51,505: <INFO> Epoch 88, loss 0.8253
2021-05-25 22:29:51,506: <INFO> Epoch 89, loss 0.8535
2021-05-25 22:29:51,506: <INFO> Epoch 90, loss 0.8355
2021-05-25 22:29:51,507: <INFO> Epoch 91, loss 0.8293
2021-05-25 22:29:51,508: <INFO> Epoch 92, loss 0.8309
2021-05-25 22:29:51,508: <INFO> Epoch 93, loss 0.8533
2021-05-25 22:29:51,509: <INFO> Epoch 94, loss 0.8418
2021-05-25 22:29:51,510: <INFO> Epoch 95, loss 0.8476
2021-05-25 22:29:51,510: <INFO> Epoch 96, loss 0.8311
2021-05-25 22:29:51,511: <INFO> Epoch 97, loss 0.8214
2021-05-25 22:29:51,511: <INFO> Epoch 98, loss 0.8200
2021-05-25 22:29:51,512: <INFO> Epoch 99, loss 0.8161
2021-05-25 22:29:51,513: <INFO> Epoch 100, loss 0.8203
2021-05-25 22:29:51,513: <INFO> Epoch 101, loss 0.8270
2021-05-25 22:29:51,514: <INFO> Epoch 102, loss 0.7999
2021-05-25 22:29:51,515: <INFO> Epoch 103, loss 0.8108
2021-05-25 22:29:51,515: <INFO> Epoch 104, loss 0.8341
2021-05-25 22:29:51,516: <INFO> Epoch 105, loss 0.8300
2021-05-25 22:29:51,516: <INFO> Epoch 106, loss 0.7935
2021-05-25 22:29:51,517: <INFO> Epoch 107, loss 0.8090
2021-05-25 22:29:51,517: <INFO> Epoch 108, loss 0.7891
2021-05-25 22:29:51,518: <INFO> Epoch 109, loss 0.8003
2021-05-25 22:29:51,520: <INFO> Epoch 110, loss 0.8204
2021-05-25 22:29:51,520: <INFO> Epoch 111, loss 0.8248
2021-05-25 22:29:51,521: <INFO> Epoch 112, loss 0.8087
2021-05-25 22:29:51,522: <INFO> Epoch 113, loss 0.7974
2021-05-25 22:29:51,523: <INFO> Epoch 114, loss 0.7857
2021-05-25 22:29:51,523: <INFO> Epoch 115, loss 0.7729
2021-05-25 22:29:51,524: <INFO> Epoch 116, loss 0.7873
2021-05-25 22:29:51,525: <INFO> Epoch 117, loss 0.7899
2021-05-25 22:29:51,526: <INFO> Epoch 118, loss 0.7976
2021-05-25 22:29:51,526: <INFO> Epoch 119, loss 0.7865
2021-05-25 22:29:51,527: <INFO> Epoch 120, loss 0.7739
2021-05-25 22:29:51,527: <INFO> Epoch 121, loss 0.8190
2021-05-25 22:29:51,528: <INFO> Epoch 122, loss 0.7988
2021-05-25 22:29:51,528: <INFO> Epoch 123, loss 0.7748
2021-05-25 22:29:51,529: <INFO> Epoch 124, loss 0.7757
2021-05-25 22:29:51,529: <INFO> Epoch 125, loss 0.7409
2021-05-25 22:29:51,530: <INFO> Epoch 126, loss 0.7782
2021-05-25 22:29:51,530: <INFO> Epoch 127, loss 0.7758
2021-05-25 22:29:51,531: <INFO> Epoch 128, loss 0.7670
2021-05-25 22:29:51,531: <INFO> Epoch 129, loss 0.7782
2021-05-25 22:29:51,532: <INFO> Epoch 130, loss 0.7629
2021-05-25 22:29:51,532: <INFO> Epoch 131, loss 0.7696
2021-05-25 22:29:51,533: <INFO> Epoch 132, loss 0.7620
2021-05-25 22:29:51,533: <INFO> Epoch 133, loss 0.7636
2021-05-25 22:29:51,534: <INFO> Epoch 134, loss 0.7429
2021-05-25 22:29:51,535: <INFO> Epoch 135, loss 0.7791
2021-05-25 22:29:51,537: <INFO> Epoch 136, loss 0.7670
2021-05-25 22:29:51,538: <INFO> Epoch 137, loss 0.7609
2021-05-25 22:29:51,539: <INFO> Epoch 138, loss 0.7495
2021-05-25 22:29:51,540: <INFO> Epoch 139, loss 0.7552
2021-05-25 22:29:51,540: <INFO> Epoch 140, loss 0.7762
2021-05-25 22:29:51,541: <INFO> Epoch 141, loss 0.7645
2021-05-25 22:29:51,542: <INFO> Epoch 142, loss 0.7636
2021-05-25 22:29:51,542: <INFO> Epoch 143, loss 0.7374
2021-05-25 22:29:51,543: <INFO> Epoch 144, loss 0.7501
2021-05-25 22:29:51,544: <INFO> Epoch 145, loss 0.7372
2021-05-25 22:29:51,544: <INFO> Epoch 146, loss 0.7476
2021-05-25 22:29:51,545: <INFO> Epoch 147, loss 0.7706
2021-05-25 22:29:51,545: <INFO> Epoch 148, loss 0.7448
2021-05-25 22:29:51,546: <INFO> Epoch 149, loss 0.7736
2021-05-25 22:29:51,546: <INFO> Epoch 150, loss 0.7658
2021-05-25 22:29:51,547: <INFO> Epoch 151, loss 0.7459
2021-05-25 22:29:51,547: <INFO> Epoch 152, loss 0.7273
2021-05-25 22:29:51,548: <INFO> Epoch 153, loss 0.7547
2021-05-25 22:29:51,549: <INFO> Epoch 154, loss 0.7589
2021-05-25 22:29:51,549: <INFO> Epoch 155, loss 0.7436
2021-05-25 22:29:51,550: <INFO> Epoch 156, loss 0.7547
2021-05-25 22:29:51,550: <INFO> Epoch 157, loss 0.7291
2021-05-25 22:29:51,550: <INFO> Epoch 158, loss 0.7337
2021-05-25 22:29:51,551: <INFO> Epoch 159, loss 0.7593
2021-05-25 22:29:51,551: <INFO> Epoch 160, loss 0.7422
2021-05-25 22:29:51,553: <INFO> Epoch 161, loss 0.7367
2021-05-25 22:29:51,554: <INFO> Epoch 162, loss 0.7446
2021-05-25 22:29:51,555: <INFO> Epoch 163, loss 0.7395
2021-05-25 22:29:51,556: <INFO> Epoch 164, loss 0.7553
2021-05-25 22:29:51,556: <INFO> Epoch 165, loss 0.7184
2021-05-25 22:29:51,557: <INFO> Epoch 166, loss 0.7418
2021-05-25 22:29:51,557: <INFO> Epoch 167, loss 0.7283
2021-05-25 22:29:51,558: <INFO> Epoch 168, loss 0.7343
2021-05-25 22:29:51,559: <INFO> Epoch 169, loss 0.7395
2021-05-25 22:29:51,559: <INFO> Epoch 170, loss 0.7216
2021-05-25 22:29:51,560: <INFO> Epoch 171, loss 0.7241
2021-05-25 22:29:51,560: <INFO> Epoch 172, loss 0.7438
2021-05-25 22:29:51,560: <INFO> Epoch 173, loss 0.7377
2021-05-25 22:29:51,561: <INFO> Epoch 174, loss 0.7169
2021-05-25 22:29:51,561: <INFO> Epoch 175, loss 0.7126
2021-05-25 22:29:51,562: <INFO> Epoch 176, loss 0.7189
2021-05-25 22:29:51,563: <INFO> Epoch 177, loss 0.7209
2021-05-25 22:29:51,563: <INFO> Epoch 178, loss 0.7278
2021-05-25 22:29:51,564: <INFO> Epoch 179, loss 0.7439
2021-05-25 22:29:51,564: <INFO> Epoch 180, loss 0.7185
2021-05-25 22:29:51,565: <INFO> Epoch 181, loss 0.7151
2021-05-25 22:29:51,565: <INFO> Epoch 182, loss 0.6886
2021-05-25 22:29:51,566: <INFO> Epoch 183, loss 0.7257
2021-05-25 22:29:51,566: <INFO> Epoch 184, loss 0.7032
2021-05-25 22:29:51,567: <INFO> Epoch 185, loss 0.7245
2021-05-25 22:29:51,568: <INFO> Epoch 186, loss 0.7121
2021-05-25 22:29:51,569: <INFO> Epoch 187, loss 0.6944
2021-05-25 22:29:51,570: <INFO> Epoch 188, loss 0.7229
2021-05-25 22:29:51,570: <INFO> Epoch 189, loss 0.7095
2021-05-25 22:29:51,570: <INFO> Epoch 190, loss 0.6958
2021-05-25 22:29:51,571: <INFO> Epoch 191, loss 0.7294
2021-05-25 22:29:51,572: <INFO> Epoch 192, loss 0.7073
2021-05-25 22:29:51,572: <INFO> Epoch 193, loss 0.7156
2021-05-25 22:29:51,573: <INFO> Epoch 194, loss 0.7195
2021-05-25 22:29:51,573: <INFO> Epoch 195, loss 0.6995
2021-05-25 22:29:51,574: <INFO> Epoch 196, loss 0.7216
2021-05-25 22:29:51,575: <INFO> Epoch 197, loss 0.7011
2021-05-25 22:29:51,575: <INFO> Epoch 198, loss 0.6904
2021-05-25 22:29:51,576: <INFO> Epoch 199, loss 0.7306
2021-05-25 22:29:51,576: <INFO> Epoch 200, loss 0.7093
2021-05-25 22:29:51,577: <INFO> Epoch 201, loss 0.6966
2021-05-25 22:29:51,578: <INFO> Epoch 202, loss 0.7260
2021-05-25 22:29:51,578: <INFO> Epoch 203, loss 0.6903
2021-05-25 22:29:51,579: <INFO> Epoch 204, loss 0.6986
2021-05-25 22:29:51,579: <INFO> Epoch 205, loss 0.6954
2021-05-25 22:29:51,579: <INFO> Epoch 206, loss 0.6857
2021-05-25 22:29:51,580: <INFO> Epoch 207, loss 0.7272
2021-05-25 22:29:51,580: <INFO> Epoch 208, loss 0.6833
2021-05-25 22:29:51,580: <INFO> Epoch 209, loss 0.6938
2021-05-25 22:29:51,581: <INFO> Epoch 210, loss 0.7006
2021-05-25 22:29:51,581: <INFO> Epoch 211, loss 0.6882
2021-05-25 22:29:51,582: <INFO> Epoch 212, loss 0.6819
2021-05-25 22:29:51,583: <INFO> Epoch 213, loss 0.6935
2021-05-25 22:29:51,583: <INFO> Epoch 214, loss 0.6802
2021-05-25 22:29:51,584: <INFO> Epoch 215, loss 0.7006
2021-05-25 22:29:51,585: <INFO> Epoch 216, loss 0.6982
2021-05-25 22:29:51,586: <INFO> Epoch 217, loss 0.6921
2021-05-25 22:29:51,588: <INFO> Epoch 218, loss 0.6842
2021-05-25 22:29:51,589: <INFO> Epoch 219, loss 0.6936
2021-05-25 22:29:51,589: <INFO> Epoch 220, loss 0.6997
2021-05-25 22:29:51,590: <INFO> Epoch 221, loss 0.7023
2021-05-25 22:29:51,590: <INFO> Epoch 222, loss 0.6660
2021-05-25 22:29:51,590: <INFO> Epoch 223, loss 0.7029
2021-05-25 22:29:51,591: <INFO> Epoch 224, loss 0.6615
2021-05-25 22:29:51,592: <INFO> Epoch 225, loss 0.6751
2021-05-25 22:29:51,593: <INFO> Epoch 226, loss 0.6812
2021-05-25 22:29:51,593: <INFO> Epoch 227, loss 0.6787
2021-05-25 22:29:51,594: <INFO> Epoch 228, loss 0.6778
2021-05-25 22:29:51,595: <INFO> Epoch 229, loss 0.7154
2021-05-25 22:29:51,595: <INFO> Epoch 230, loss 0.6730
2021-05-25 22:29:51,596: <INFO> Epoch 231, loss 0.7109
2021-05-25 22:29:51,596: <INFO> Epoch 232, loss 0.6853
2021-05-25 22:29:51,597: <INFO> Epoch 233, loss 0.6812
2021-05-25 22:29:51,597: <INFO> Epoch 234, loss 0.6874
2021-05-25 22:29:51,598: <INFO> Epoch 235, loss 0.6803
2021-05-25 22:29:51,598: <INFO> Epoch 236, loss 0.6827
2021-05-25 22:29:51,599: <INFO> Epoch 237, loss 0.6755
2021-05-25 22:29:51,599: <INFO> Epoch 238, loss 0.6716
2021-05-25 22:29:51,600: <INFO> Epoch 239, loss 0.6874
2021-05-25 22:29:51,600: <INFO> Epoch 240, loss 0.6572
2021-05-25 22:29:51,600: <INFO> Epoch 241, loss 0.6855
2021-05-25 22:29:51,602: <INFO> Epoch 242, loss 0.6729
2021-05-25 22:29:51,603: <INFO> Epoch 243, loss 0.6768
2021-05-25 22:29:51,603: <INFO> Epoch 244, loss 0.6660
2021-05-25 22:29:51,604: <INFO> Epoch 245, loss 0.6898
2021-05-25 22:29:51,605: <INFO> Epoch 246, loss 0.6786
2021-05-25 22:29:51,606: <INFO> Epoch 247, loss 0.6900
2021-05-25 22:29:51,607: <INFO> Epoch 248, loss 0.6511
2021-05-25 22:29:51,608: <INFO> Epoch 249, loss 0.6746
2021-05-25 22:29:51,608: <INFO> Epoch 250, loss 0.6395
2021-05-25 22:29:51,609: <INFO> Epoch 251, loss 0.6833
2021-05-25 22:29:51,610: <INFO> Epoch 252, loss 0.6739
2021-05-25 22:29:51,610: <INFO> Epoch 253, loss 0.6788
2021-05-25 22:29:51,611: <INFO> Epoch 254, loss 0.6974
2021-05-25 22:29:51,612: <INFO> Epoch 255, loss 0.6475
2021-05-25 22:29:51,613: <INFO> Epoch 256, loss 0.6697
2021-05-25 22:29:51,613: <INFO> Epoch 257, loss 0.6471
2021-05-25 22:29:51,614: <INFO> Epoch 258, loss 0.6510
2021-05-25 22:29:51,615: <INFO> Epoch 259, loss 0.6944
2021-05-25 22:29:51,615: <INFO> Epoch 260, loss 0.6923
2021-05-25 22:29:51,616: <INFO> Epoch 261, loss 0.6843
2021-05-25 22:29:51,616: <INFO> Epoch 262, loss 0.6926
2021-05-25 22:29:51,617: <INFO> Epoch 263, loss 0.6631
2021-05-25 22:29:51,618: <INFO> Epoch 264, loss 0.6764
2021-05-25 22:29:51,619: <INFO> Epoch 265, loss 0.6935
2021-05-25 22:29:51,620: <INFO> Epoch 266, loss 0.6731
2021-05-25 22:29:51,621: <INFO> Epoch 267, loss 0.6574
2021-05-25 22:29:51,621: <INFO> Epoch 268, loss 0.6788
2021-05-25 22:29:51,622: <INFO> Epoch 269, loss 0.6476
2021-05-25 22:29:51,623: <INFO> Epoch 270, loss 0.6702
2021-05-25 22:29:51,623: <INFO> Epoch 271, loss 0.6763
2021-05-25 22:29:51,624: <INFO> Epoch 272, loss 0.6799
2021-05-25 22:29:51,624: <INFO> Epoch 273, loss 0.6785
2021-05-25 22:29:51,625: <INFO> Epoch 274, loss 0.6721
2021-05-25 22:29:51,626: <INFO> Epoch 275, loss 0.6591
2021-05-25 22:29:51,626: <INFO> Epoch 276, loss 0.6615
2021-05-25 22:29:51,627: <INFO> Epoch 277, loss 0.6953
2021-05-25 22:29:51,627: <INFO> Epoch 278, loss 0.6468
2021-05-25 22:29:51,628: <INFO> Epoch 279, loss 0.6731
2021-05-25 22:29:51,628: <INFO> Epoch 280, loss 0.6472
2021-05-25 22:29:51,629: <INFO> Epoch 281, loss 0.6768
2021-05-25 22:29:51,629: <INFO> Epoch 282, loss 0.6513
2021-05-25 22:29:51,630: <INFO> Epoch 283, loss 0.6373
2021-05-25 22:29:51,630: <INFO> Epoch 284, loss 0.6588
2021-05-25 22:29:51,630: <INFO> Epoch 285, loss 0.6311
2021-05-25 22:29:51,631: <INFO> Epoch 286, loss 0.6517
2021-05-25 22:29:51,632: <INFO> Epoch 287, loss 0.6559
2021-05-25 22:29:51,633: <INFO> Epoch 288, loss 0.6578
2021-05-25 22:29:51,633: <INFO> Epoch 289, loss 0.6555
2021-05-25 22:29:51,634: <INFO> Epoch 290, loss 0.6558
2021-05-25 22:29:51,634: <INFO> Epoch 291, loss 0.6613
2021-05-25 22:29:51,636: <INFO> Epoch 292, loss 0.6449
2021-05-25 22:29:51,637: <INFO> Epoch 293, loss 0.6518
2021-05-25 22:29:51,638: <INFO> Epoch 294, loss 0.6724
2021-05-25 22:29:51,639: <INFO> Epoch 295, loss 0.6472
2021-05-25 22:29:51,639: <INFO> Epoch 296, loss 0.6379
2021-05-25 22:29:51,639: <INFO> Epoch 297, loss 0.6448
2021-05-25 22:29:51,640: <INFO> Epoch 298, loss 0.6439
2021-05-25 22:29:51,640: <INFO> Epoch 299, loss 0.6298
2021-05-25 22:29:51,640: <INFO> Epoch 300, loss 0.6618
2021-05-25 22:29:51,641: <INFO> Epoch 301, loss 0.6663
2021-05-25 22:29:51,641: <INFO> Epoch 302, loss 0.6263
2021-05-25 22:29:51,642: <INFO> Epoch 303, loss 0.6640
2021-05-25 22:29:51,643: <INFO> Epoch 304, loss 0.6688
2021-05-25 22:29:51,644: <INFO> Epoch 305, loss 0.6446
2021-05-25 22:29:51,644: <INFO> Epoch 306, loss 0.6666
2021-05-25 22:29:51,645: <INFO> Epoch 307, loss 0.6339
2021-05-25 22:29:51,646: <INFO> Epoch 308, loss 0.6468
2021-05-25 22:29:51,647: <INFO> Epoch 309, loss 0.6427
2021-05-25 22:29:51,647: <INFO> Epoch 310, loss 0.6651
2021-05-25 22:29:51,648: <INFO> Epoch 311, loss 0.6420
2021-05-25 22:29:51,649: <INFO> Epoch 312, loss 0.6732
2021-05-25 22:29:51,649: <INFO> Epoch 313, loss 0.6673
2021-05-25 22:29:51,650: <INFO> Epoch 314, loss 0.6528
2021-05-25 22:29:51,650: <INFO> Epoch 315, loss 0.6478
2021-05-25 22:29:51,650: <INFO> Epoch 316, loss 0.6509
2021-05-25 22:29:51,650: <INFO> Epoch 317, loss 0.6565
2021-05-25 22:29:51,652: <INFO> Epoch 318, loss 0.6667
2021-05-25 22:29:51,653: <INFO> Epoch 319, loss 0.6397
2021-05-25 22:29:51,654: <INFO> Epoch 320, loss 0.6314
2021-05-25 22:29:51,655: <INFO> Epoch 321, loss 0.6656
2021-05-25 22:29:51,656: <INFO> Epoch 322, loss 0.6298
2021-05-25 22:29:51,656: <INFO> Epoch 323, loss 0.6436
2021-05-25 22:29:51,657: <INFO> Epoch 324, loss 0.6431
2021-05-25 22:29:51,657: <INFO> Epoch 325, loss 0.6585
2021-05-25 22:29:51,658: <INFO> Epoch 326, loss 0.6451
2021-05-25 22:29:51,659: <INFO> Epoch 327, loss 0.6506
2021-05-25 22:29:51,659: <INFO> Epoch 328, loss 0.6449
2021-05-25 22:29:51,660: <INFO> Epoch 329, loss 0.6553
2021-05-25 22:29:51,660: <INFO> Epoch 330, loss 0.6468
2021-05-25 22:29:51,660: <INFO> Epoch 331, loss 0.6332
2021-05-25 22:29:51,661: <INFO> Epoch 332, loss 0.6231
2021-05-25 22:29:51,662: <INFO> Epoch 333, loss 0.6516
2021-05-25 22:29:51,662: <INFO> Epoch 334, loss 0.6267
2021-05-25 22:29:51,663: <INFO> Epoch 335, loss 0.6540
2021-05-25 22:29:51,664: <INFO> Epoch 336, loss 0.6779
2021-05-25 22:29:51,664: <INFO> Epoch 337, loss 0.6558
2021-05-25 22:29:51,665: <INFO> Epoch 338, loss 0.6545
2021-05-25 22:29:51,666: <INFO> Epoch 339, loss 0.6504
2021-05-25 22:29:51,666: <INFO> Epoch 340, loss 0.6522
2021-05-25 22:29:51,667: <INFO> Epoch 341, loss 0.6444
2021-05-25 22:29:51,667: <INFO> Epoch 342, loss 0.6636
2021-05-25 22:29:51,668: <INFO> Epoch 343, loss 0.6553
2021-05-25 22:29:51,669: <INFO> Epoch 344, loss 0.6347
2021-05-25 22:29:51,670: <INFO> Epoch 345, loss 0.6271
2021-05-25 22:29:51,670: <INFO> Epoch 346, loss 0.6358
2021-05-25 22:29:51,670: <INFO> Epoch 347, loss 0.6305
2021-05-25 22:29:51,671: <INFO> Epoch 348, loss 0.6473
2021-05-25 22:29:51,672: <INFO> Epoch 349, loss 0.6520
2021-05-25 22:29:51,673: <INFO> Epoch 350, loss 0.6289
2021-05-25 22:29:51,674: <INFO> Epoch 351, loss 0.6494
2021-05-25 22:29:51,674: <INFO> Epoch 352, loss 0.6286
2021-05-25 22:29:51,675: <INFO> Epoch 353, loss 0.6573
2021-05-25 22:29:51,676: <INFO> Epoch 354, loss 0.6335
2021-05-25 22:29:51,677: <INFO> Epoch 355, loss 0.6355
2021-05-25 22:29:51,678: <INFO> Epoch 356, loss 0.6369
2021-05-25 22:29:51,678: <INFO> Epoch 357, loss 0.6282
2021-05-25 22:29:51,678: <INFO> Epoch 358, loss 0.6450
2021-05-25 22:29:51,679: <INFO> Epoch 359, loss 0.6233
2021-05-25 22:29:51,679: <INFO> Epoch 360, loss 0.6371
2021-05-25 22:29:51,680: <INFO> Epoch 361, loss 0.6424
2021-05-25 22:29:51,680: <INFO> Epoch 362, loss 0.6427
2021-05-25 22:29:51,680: <INFO> Epoch 363, loss 0.6314
2021-05-25 22:29:51,680: <INFO> Epoch 364, loss 0.6546
2021-05-25 22:29:51,681: <INFO> Epoch 365, loss 0.6373
2021-05-25 22:29:51,681: <INFO> Epoch 366, loss 0.6514
2021-05-25 22:29:51,681: <INFO> Epoch 367, loss 0.6349
2021-05-25 22:29:51,682: <INFO> Epoch 368, loss 0.6467
2021-05-25 22:29:51,682: <INFO> Epoch 369, loss 0.6357
2021-05-25 22:29:51,682: <INFO> Epoch 370, loss 0.6174
2021-05-25 22:29:51,683: <INFO> Epoch 371, loss 0.6408
2021-05-25 22:29:51,684: <INFO> Epoch 372, loss 0.6318
2021-05-25 22:29:51,684: <INFO> Epoch 373, loss 0.6275
2021-05-25 22:29:51,684: <INFO> Epoch 374, loss 0.6300
2021-05-25 22:29:51,685: <INFO> Epoch 375, loss 0.6240
2021-05-25 22:29:51,686: <INFO> Epoch 376, loss 0.6249
2021-05-25 22:29:51,687: <INFO> Epoch 377, loss 0.6228
2021-05-25 22:29:51,688: <INFO> Epoch 378, loss 0.6640
2021-05-25 22:29:51,688: <INFO> Epoch 379, loss 0.6455
2021-05-25 22:29:51,689: <INFO> Epoch 380, loss 0.6246
2021-05-25 22:29:51,689: <INFO> Epoch 381, loss 0.6353
2021-05-25 22:29:51,690: <INFO> Epoch 382, loss 0.6285
2021-05-25 22:29:51,690: <INFO> Epoch 383, loss 0.6465
2021-05-25 22:29:51,690: <INFO> Epoch 384, loss 0.6200
2021-05-25 22:29:51,690: <INFO> Epoch 385, loss 0.6168
2021-05-25 22:29:51,691: <INFO> Epoch 386, loss 0.6270
2021-05-25 22:29:51,691: <INFO> Epoch 387, loss 0.6241
2021-05-25 22:29:51,691: <INFO> Epoch 388, loss 0.6072
2021-05-25 22:29:51,692: <INFO> Epoch 389, loss 0.6039
2021-05-25 22:29:51,692: <INFO> Epoch 390, loss 0.6283
2021-05-25 22:29:51,693: <INFO> Epoch 391, loss 0.6188
2021-05-25 22:29:51,693: <INFO> Epoch 392, loss 0.6288
2021-05-25 22:29:51,694: <INFO> Epoch 393, loss 0.6516
2021-05-25 22:29:51,694: <INFO> Epoch 394, loss 0.6095
2021-05-25 22:29:51,695: <INFO> Epoch 395, loss 0.6290
2021-05-25 22:29:51,695: <INFO> Epoch 396, loss 0.6341
2021-05-25 22:29:51,695: <INFO> Epoch 397, loss 0.6262
2021-05-25 22:29:51,696: <INFO> Epoch 398, loss 0.6214
2021-05-25 22:29:51,697: <INFO> Epoch 399, loss 0.6250
2021-05-25 22:29:52,037: <WARNING> From C:\Users\Teucros\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\training\tracking\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2021-05-25 22:29:52,053: <WARNING> From C:\Users\Teucros\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\training\tracking\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2021-05-25 22:29:53,240: <INFO> Assets written to: trained_models/efficent\assets
2021-05-25 22:29:53,285: <INFO> Best loss is 0.624963
2021-05-26 09:26:29,310: <INFO> Trying efficent model
2021-05-26 09:28:29,372: <INFO> Trying efficent model
2021-05-26 09:34:00,135: <INFO> Trying efficent model
2021-05-26 09:42:26,320: <INFO> Trying efficent model
2021-05-26 09:42:26,326: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:43:23,430: <INFO> Trying efficent model
2021-05-26 09:43:23,436: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:43:32,207: <ERROR> Uncaught exception
Traceback (most recent call last):
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy\__main__.py", line 45, in <module>
    cli.main()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str("__main__"))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 96, in <module>
    try_model("efficent", 224, 224, 64, efficient.preprocess, efficient.train_model, script_args.validate)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 78, in try_model
    dataset = data.noisy_dataset("../coco/2017/train/", 154, 359, patch_size, patch_stride, batch_size)
  File "F:\git\deep\dnn-noise-estimation\data.py", line 86, in noisy_dataset
    return tf.data.Dataset.from_generator(generator, output_signature = output_signature)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\util\deprecation.py", line 535, in new_func
    return func(*args, **kwargs)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 877, in from_generator
    raise TypeError("`generator` must be callable.")
TypeError: `generator` must be callable.
2021-05-26 09:46:13,991: <INFO> Trying efficent model
2021-05-26 09:46:13,997: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:47:39,452: <INFO> Trying efficent model
2021-05-26 09:47:39,457: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:47:46,792: <ERROR> Uncaught exception
Traceback (most recent call last):
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy\__main__.py", line 45, in <module>
    cli.main()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str("__main__"))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 96, in <module>
    try_model("efficent", 224, 224, 64, efficient.preprocess, efficient.train_model, script_args.validate)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 78, in try_model
    dataset = data.noisy_dataset("../coco/2017/train/", 154, 359, patch_size, patch_stride, batch_size)
  File "F:\git\deep\dnn-noise-estimation\data.py", line 86, in noisy_dataset
    return tf.data.Dataset.from_generator(generator, output_signature = output_signature)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\util\deprecation.py", line 535, in new_func
    return func(*args, **kwargs)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 877, in from_generator
    raise TypeError("`generator` must be callable.")
TypeError: `generator` must be callable.
2021-05-26 09:49:04,168: <INFO> Trying efficent model
2021-05-26 09:49:04,174: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:50:11,706: <INFO> Trying efficent model
2021-05-26 09:50:11,712: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:50:18,970: <ERROR> Uncaught exception
Traceback (most recent call last):
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy\__main__.py", line 45, in <module>
    cli.main()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str("__main__"))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 96, in <module>
    try_model("efficent", 224, 224, 64, efficient.preprocess, efficient.train_model, script_args.validate)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 81, in try_model
    accuracy = estimator.evaluate(dataset)
  File "F:\git\deep\dnn-noise-estimation\noise_estimator.py", line 59, in evaluate
    prepared_dataset = dataset.map(self._preprocessing)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 1925, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 4487, in __init__
    use_legacy_function=use_legacy_function)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 3712, in __init__
    self._function = fn_factory()
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 3135, in get_concrete_function
    *args, **kwargs)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 3100, in _get_concrete_function_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 3289, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\framework\func_graph.py", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 3687, in wrapped_fn
    ret = wrapper_helper(*args)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 3617, in wrapper_helper
    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\autograph\impl\api.py", line 695, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:


    TypeError: tf__preprocess() takes 1 positional argument but 2 were given

2021-05-26 09:53:17,722: <INFO> Trying efficent model
2021-05-26 09:53:17,729: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:54:43,828: <INFO> Trying efficent model
2021-05-26 09:54:43,834: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 09:56:51,899: <INFO> Trying efficent model
2021-05-26 09:56:51,905: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:01:37,685: <INFO> Trying efficent model
2021-05-26 10:01:37,691: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:02:29,040: <INFO> Trying efficent model
2021-05-26 10:02:29,046: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:03:19,186: <INFO> Trying efficent model
2021-05-26 10:03:19,192: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:03:30,203: <ERROR> Uncaught exception
Traceback (most recent call last):
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy\__main__.py", line 45, in <module>
    cli.main()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str("__main__"))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 96, in <module>
    try_model("efficent", 224, 224, 64, efficient.preprocess, efficient.train_model, script_args.validate)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 81, in try_model
    accuracy = estimator.evaluate(dataset)
  File "F:\git\deep\dnn-noise-estimation\noise_estimator.py", line 62, in evaluate
    metrics = self._model.evaluate(prepared_dataset, verbose = 0, return_dict=True)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\keras\engine\training.py", line 1489, in evaluate
    tmp_logs = self.test_function(iterator)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\def_function.py", line 889, in __call__
    result = self._call(*args, **kwds)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\def_function.py", line 957, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 1961, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 596, in call
    ctx=ctx)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcd in position 81: invalid continuation byte
Traceback (most recent call last):

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\script_ops.py", line 249, in __call__
    ret = func(*args)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\autograph\impl\api.py", line 645, in wrapper
    return func(*args, **kwargs)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 961, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File "F:\git\deep\dnn-noise-estimation\data.py", line 31, in generator
    source_image = load_image(image_path)

  File "F:\git\deep\dnn-noise-estimation\data.py", line 5, in load_image
    image_file = tf.io.read_file(str(path))

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\io_ops.py", line 138, in read_file
    return gen_io_ops.read_file(filename, name)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 555, in read_file
    filename, name=name, ctx=_ctx)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 578, in read_file_eager_fallback
    attrs=_attrs, ctx=ctx, name=name)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcd in position 81: invalid continuation byte


	 [[{{node PyFunc}}]]
	 [[IteratorGetNext]] [Op:__inference_test_function_6454]

Function call stack:
test_function

2021-05-26 10:04:54,329: <INFO> Trying efficent model
2021-05-26 10:04:54,336: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:05:01,620: <ERROR> Uncaught exception
Traceback (most recent call last):
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy\__main__.py", line 45, in <module>
    cli.main()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str("__main__"))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 96, in <module>
    try_model("efficent", 224, 224, 64, efficient.preprocess, efficient.train_model, script_args.validate)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 81, in try_model
    accuracy = estimator.evaluate(dataset)
  File "F:\git\deep\dnn-noise-estimation\noise_estimator.py", line 62, in evaluate
    metrics = self._model.evaluate(prepared_dataset, verbose = 0, return_dict=True)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\keras\engine\training.py", line 1489, in evaluate
    tmp_logs = self.test_function(iterator)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\def_function.py", line 889, in __call__
    result = self._call(*args, **kwds)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\def_function.py", line 957, in _call
    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 1961, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\function.py", line 596, in call
    ctx=ctx)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcd in position 81: invalid continuation byte
Traceback (most recent call last):

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\script_ops.py", line 249, in __call__
    ret = func(*args)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\autograph\impl\api.py", line 645, in wrapper
    return func(*args, **kwargs)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 961, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File "F:\git\deep\dnn-noise-estimation\data.py", line 31, in generator
    source_image = load_image(image_path)

  File "F:\git\deep\dnn-noise-estimation\data.py", line 5, in load_image
    image_file = tf.io.read_file(str(path))

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\io_ops.py", line 138, in read_file
    return gen_io_ops.read_file(filename, name)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 555, in read_file
    filename, name=name, ctx=_ctx)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 578, in read_file_eager_fallback
    attrs=_attrs, ctx=ctx, name=name)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcd in position 81: invalid continuation byte


	 [[{{node PyFunc}}]]
	 [[IteratorGetNext]] [Op:__inference_test_function_6454]

Function call stack:
test_function

2021-05-26 10:06:08,929: <INFO> Trying efficent model
2021-05-26 10:06:08,935: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:07:12,938: <INFO> Trying efficent model
2021-05-26 10:07:12,945: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:08:45,824: <ERROR> Uncaught exception
Traceback (most recent call last):
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy\__main__.py", line 45, in <module>
    cli.main()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str("__main__"))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 96, in <module>
    try_model("efficent", 224, 224, 64, efficient.preprocess, efficient.train_model, script_args.validate)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 78, in try_model
    dataset = data.noisy_dataset("../coco/2017/train/", 154, 359, patch_size, patch_stride, batch_size)
  File "F:\git\deep\dnn-noise-estimation\data.py", line 71, in noisy_dataset
    for x in generator():
  File "F:\git\deep\dnn-noise-estimation\data.py", line 31, in generator
    source_image = load_image(image_path)
  File "F:\git\deep\dnn-noise-estimation\data.py", line 5, in load_image
    image_file = tf.io.read_file(str(path))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\io_ops.py", line 138, in read_file
    return gen_io_ops.read_file(filename, name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 555, in read_file
    filename, name=name, ctx=_ctx)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 578, in read_file_eager_fallback
    attrs=_attrs, ctx=ctx, name=name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcd in position 81: invalid continuation byte
2021-05-26 10:09:12,164: <INFO> Trying efficent model
2021-05-26 10:09:12,171: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:10:00,447: <INFO> Trying efficent model
2021-05-26 10:10:00,454: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:10:08,783: <ERROR> Uncaught exception
Traceback (most recent call last):
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy\__main__.py", line 45, in <module>
    cli.main()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\program files (x86)\microsoft visual studio\2019\enterprise\common7\ide\extensions\microsoft\python\core\debugpy/..\debugpy\server\cli.py", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str("__main__"))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 96, in <module>
    try_model("efficent", 224, 224, 64, efficient.preprocess, efficient.train_model, script_args.validate)
  File "F:\git\deep\dnn-noise-estimation\dnn_noise_estimation.py", line 78, in try_model
    dataset = data.noisy_dataset("../coco/2017/train/", 154, 359, patch_size, patch_stride, batch_size)
  File "F:\git\deep\dnn-noise-estimation\data.py", line 71, in noisy_dataset
    for x in generator():
  File "F:\git\deep\dnn-noise-estimation\data.py", line 31, in generator
    source_image = load_image(image_path)
  File "F:\git\deep\dnn-noise-estimation\data.py", line 5, in load_image
    image_file = tf.io.read_file(str(path))
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\io_ops.py", line 138, in read_file
    return gen_io_ops.read_file(filename, name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 555, in read_file
    filename, name=name, ctx=_ctx)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 578, in read_file_eager_fallback
    attrs=_attrs, ctx=ctx, name=name)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\lib\site-packages\tensorflow\python\eager\execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcd in position 81: invalid continuation byte
2021-05-26 10:10:39,205: <INFO> Trying efficent model
2021-05-26 10:10:39,211: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:11:21,559: <INFO> Trying efficent model
2021-05-26 10:11:21,567: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:12:07,150: <INFO> Trying efficent model
2021-05-26 10:12:07,157: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:14:55,225: <INFO> Trying efficent model
2021-05-26 10:14:55,232: <WARNING> SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
2021-05-26 10:16:29,710: <INFO> Accuracy is 45.4%
